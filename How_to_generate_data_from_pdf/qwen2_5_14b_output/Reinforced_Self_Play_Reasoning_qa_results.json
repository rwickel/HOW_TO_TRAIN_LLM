[
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 1,
    "page_text": "arXiv:2505.03335v2  [cs.LG]  7 May 2025\nMay 16, 2025\nAbsolute Zero: Reinforced Self-play Reasoning with Zero Data\nAndrew Zhao 1,\nYiran Wu 3,\nYang Yue 1,\nTong Wu 2,\nQuentin Xu 1,\nYang Yue 1,\nMatthieu Lin 1,\nShenzhi Wang 1, Qingyun Wu 3, Zilong Zheng 2,\u0000 and Gao Huang 1,\u00001 Tsinghua University\n2 Beĳing Institute for General Artificial Intelligence\n3 Pennsylvania State University\nzqc21@mails.tsinghua.edu.cn, yiran.wu@psu.edu, zlzheng@bigai.ai, gaohuang@tsinghua.edu.cn\nReinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning\ncapabilities of large language models by learning directly from outcome-based rewards. Recent RLVR\nworks that operate under the zero setting avoid supervision in labeling the reasoning process, but still\ndepend on manually curated collections of questions and answers for training. The scarcity of high-\nquality, human-produced examples raises concerns about the long-term scalability of relying on human\nsupervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a\nhypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited\nlearning potential for a superintelligent system. To address these concerns, we propose a new RLVR\nparadigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own\nlearning progress and improves reasoning by solving them, without relying on any external data. Under\nthis paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training\ncurriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks\nand verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded\nlearning. Despite being trained entirely without external data, AZR achieves overall SOTA performance\non coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens\nof thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be\neffectively applied across different model scales and is compatible with various model classes.\nCode\nProject Page\nLogs\nModels\nFigure 1. Absolute Zero Reasoner (AZR) achieves state-of-the-art performance with ZERO DATA. Without relying on any gold\nlabels or human-defined queries, Absolute Zero Reasoner trained using our proposed self-play approach demonstrates impressive general\nreasoning capabilities improvements in both math and coding, despite operating entirely out-of-distribution. Remarkably, AZR surpasses\nmodels trained on tens of thousands of expert-labeled in-domain examples in the combined average score across both domains.\n\u0000 Corresponding author(s)",
    "qa_pairs": [
      {
        "question": "What is the main goal of Absolute Zero Reasoner (AZR)?",
        "answer": "The main goal of AZR is to enable a single model to propose tasks that maximize its own learning progress and improve reasoning capabilities without relying on any external data or human supervision (see Reinforced_Self_Play_Reasoning, Introduction)."
      },
      {
        "question": "How does Absolute Zero Reasoner achieve self-evolution in training?",
        "answer": "AZR achieves self-evolution by using a code executor to validate proposed reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide its learning process (see Reinforced_Self_Play_Reasoning, Introduction)."
      },
      {
        "question": "What is the performance benchmark for AZR?",
        "answer": "AZR achieves state-of-the-art performance on coding and mathematical reasoning tasks without any external data or human-defined queries, surpassing models trained on tens of thousands of expert-labeled in-domain examples (see Reinforced_Self_Play_Reasoning, Introduction)."
      },
      {
        "question": "What is the significance of AZR's approach to reinforcement learning?",
        "answer": "AZR's approach addresses the limitations of existing RLVR methods by eliminating the need for manually curated collections of questions and answers, thus overcoming scalability issues related to long-term reliance on human supervision (see Reinforced_Self_Play_Reasoning, Introduction)."
      },
      {
        "question": "How does AZR demonstrate its effectiveness in different model scales?",
        "answer": "AZR demonstrates that it can be effectively applied across various model scales and is compatible with multiple model classes, showcasing its versatility and robustness (see Reinforced_Self_Play_Reasoning, Introduction)."
      }
    ],
    "raw_response": "[\n  {\n    \"question\": \"What is the main goal of Absolute Zero Reasoner (AZR)?\",\n    \"answer\": \"The main goal of AZR is to enable a single model to propose tasks that maximize its own learning progress and improve reasoning capabilities without relying on any external data or human supervision (see Reinforced_Self_Play_Reasoning, Introduction).\"\n  },\n  {\n    \"question\": \"How does Absolute Zero Reasoner achieve self-evolution in training?\",\n    \"answer\": \"AZR achieves self-evolution by using a code executor to validate proposed reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide its learning process (see Reinforced_Self_Play_Reasoning, Introduction).\"\n  },\n  {\n    \"question\": \"What is the performance benchmark for AZR?\",\n    \"answer\": \"AZR achieves state-of-the-art performance on coding and mathematical reasoning tasks without any external data or human-defined queries, surpassing models trained on tens of thousands of expert-labeled in-domain examples (see Reinforced_Self_Play_Reasoning, Introduction).\"\n  },\n  {\n    \"question\": \"What is the significance of AZR's approach to reinforcement learning?\",\n    \"answer\": \"AZR's approach addresses the limitations of existing RLVR methods by eliminating the need for manually curated collections of questions and answers, thus overcoming scalability issues related to long-term reliance on human supervision (see Reinforced_Self_Play_Reasoning, Introduction).\"\n  },\n  {\n    \"question\": \"How does AZR demonstrate its effectiveness in different model scales?\",\n    \"answer\": \"AZR demonstrates that it can be effectively applied across various model scales and is compatible with multiple model classes, showcasing its versatility and robustness (see Reinforced_Self_Play_Reasoning, Introduction).\"\n  }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 2,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nReinforcement Learning with Verifiable Rewards\nAbsolute Zero (Ours)\nSupervised Learning\nLess Human Supervision\nFigure 2. Absolute Zero Paradigm. Supervised learning relies on human-curated reasoning traces for behavior cloning. Reinforcement\nlearning from verified rewards, enables agents to self-learn reasoning, but still depends on expert-defined learning distribution and a\nrespective set of curated QA pairs, demanding domain expertise and manual effort. In contrast, we introduce a new paradigm, Absolute\nZero, for training reasoning models without any human-curated data. We envision that the agent should autonomously propose tasks\noptimized for learnability and learn how to solve them using an unified model. The agent learns by interacting with an environment that\nprovides verifiable feedback, enabling reliable and continuous self-improvement entirely without human intervention.\n1. Introduction\nLarge language models (LLMs) have recently achieved remarkable improvements in reasoning capabilities by employing Reinforcement\nLearning with Verifiable Rewards (RLVR) (Lambert et al., 2024). Unlike methods that explicitly imitate intermediate reasoning steps,\nRLVR uses only outcome-based feedback, enabling large-scale reinforcement learning over vast task datasets (DeepSeek-AI et al., 2025;\nTeam et al., 2025; Jaech et al., 2024; OpenAI, 2025b;a). A particularly compelling variant is the “zero” RLVR paradigm (DeepSeek-AI\net al., 2025), which forgoes any cold-start distillation data, using neither human-generated nor AI-generated reasoning traces, and applies\nRLVR directly on the base model with task rewards. However, these methods still depend heavily on expertly curated distributions of\nreasoning question–answer pairs, which raises serious concerns about their long-term scalability (Villalobos et al., 2024). As reasoning\nmodels continue to advance, the effort required to construct large-scale, high-quality datasets may soon become unsustainable (Yue\net al., 2025). A similar scalability bottleneck has already been identified in the domain of LLM pretraining (Sutskever et al., 2024).\nFurthermore, as AI systems continue to evolve and potentially exceed human intellect, an exclusive dependence on human-designed\ntasks risks imposing constraints on their capacity for autonomous learning and growth (Hughes et al., 2024). This underscores the need\nfor a new paradigm that begins to explore possibilities beyond the constraints of human-designed tasks and prepares for a future in which\nAI systems may surpass human intelligence.\nTo this end, we propose “Absolute Zero”, a new paradigm for reasoning models in which the model simultaneously learns to define tasks\nthat maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In\ncontrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to\nhacking (Silver et al., 2017; Chen et al., 2025; 2024), the Absolute Zero paradigm is designed to operate in open-ended settings while\nremaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how\nhumans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models (Hughes\net al., 2024). Similar to AlphaZero (Silver et al., 2017), which improves through self-play, our proposed paradigm requires no human\nsupervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward\nenabling large language models to autonomously achieve superhuman reasoning capabilities.\nBuilding on this new reasoning paradigm, we introduce the Absolute Zero Reasoner (AZR), which proposes and solves coding tasks. We\ncast code executor as an open-ended yet grounded environment, sufficient to both validate task integrity and also provide verifiable\nfeedback for stable training. We let AZR construct three types of coding tasks: infer and reason about one particular element in a\nprogram, input, output triplet, which corresponds to three complementary modes of reasoning: induction, abduction, and deduction. We\ntrain the entire system end-to-end with a newly proposed reinforcement learning advantage estimator tailored to the multitask nature of\nthe proposed approach.\nDespite being trained entirely without any in-distribution data, AZR demonstrates remarkable capabilities across diverse reasoning tasks\nin math and coding. In mathematics, AZR achieves competitive performance compared to zero reasoner models explicitly fine-tuned\nwith domain-specific supervision. In coding tasks, AZR establishes a new state-of-the-art performance, surpassing models specifically\ntrained with code datasets using RLVR. Furthermore, AZR outperforms all previous models by an average of 1.8 absolute points\n2",
    "qa_pairs": [
      {
        "question": "What is the main goal of Reinforcement Learning with Verifiable Rewards (RLVR) in the context of Reinforced_Self_Play_Reasoning?",
        "answer": "The main goal of RLVR is to enable large language models to improve their reasoning capabilities by using outcome-based feedback, allowing for large-scale reinforcement learning over vast task datasets without explicitly imitating intermediate reasoning steps (see Reinforced_Self_Play_Reasoning page 2)."
      },
      {
        "question": "What are the limitations of current RLVR methods as described in Reinforced_Self_Play_Reasoning?",
        "answer": "Current RLVR methods still depend heavily on expertly curated distributions of reasoning question-answer pairs, which raises concerns about long-term scalability and the ability for AI systems to autonomously learn beyond human-designed tasks (see Reinforced_Self_Play_Reasoning page 2)."
      },
      {
        "question": "What is the Absolute Zero paradigm introduced in Reinforced_Self_Play_Reasoning?",
        "answer": "The Absolute Zero paradigm is a new approach for reasoning models that enables self-evolution through self-play without relying on external data, allowing agents to define and solve tasks autonomously (see Reinforced_Self_Play_Reasoning page 2)."
      },
      {
        "question": "How does the Absolute Zero Reasoner (AZR) operate in the context of coding tasks?",
        "answer": "The AZR constructs three types of coding tasks: inferring and reasoning about one particular element in a program, input, output triplet, corresponding to induction, abduction, and deduction modes of reasoning. It operates within an environment that validates task integrity and provides verifiable feedback for stable training (see Reinforced_Self_Play_Reasoning page 2)."
      },
      {
        "question": "What is the significance of AZR's performance in diverse reasoning tasks according to Reinforced_Self_Play_Reasoning?",
        "answer": "AZR demonstrates remarkable capabilities across various reasoning tasks, achieving competitive performance in mathematics and establishing a new state-of-the-art performance in coding tasks without any in-distribution data (see Reinforced_Self_Play_Reasoning page 2)."
      }
    ],
    "raw_response": "[\n  {\n    \"question\": \"What is the main goal of Reinforcement Learning with Verifiable Rewards (RLVR) in the context of Reinforced_Self_Play_Reasoning?\",\n    \"answer\": \"The main goal of RLVR is to enable large language models to improve their reasoning capabilities by using outcome-based feedback, allowing for large-scale reinforcement learning over vast task datasets without explicitly imitating intermediate reasoning steps (see Reinforced_Self_Play_Reasoning page 2).\"\n  },\n  {\n    \"question\": \"What are the limitations of current RLVR methods as described in Reinforced_Self_Play_Reasoning?\",\n    \"answer\": \"Current RLVR methods still depend heavily on expertly curated distributions of reasoning question-answer pairs, which raises concerns about long-term scalability and the ability for AI systems to autonomously learn beyond human-designed tasks (see Reinforced_Self_Play_Reasoning page 2).\"\n  },\n  {\n    \"question\": \"What is the Absolute Zero paradigm introduced in Reinforced_Self_Play_Reasoning?\",\n    \"answer\": \"The Absolute Zero paradigm is a new approach for reasoning models that enables self-evolution through self-play without relying on external data, allowing agents to define and solve tasks autonomously (see Reinforced_Self_Play_Reasoning page 2).\"\n  },\n  {\n    \"question\": \"How does the Absolute Zero Reasoner (AZR) operate in the context of coding tasks?\",\n    \"answer\": \"The AZR constructs three types of coding tasks: inferring and reasoning about one particular element in a program, input, output triplet, corresponding to induction, abduction, and deduction modes of reasoning. It operates within an environment that validates task integrity and provides verifiable feedback for stable training (see Reinforced_Self_Play_Reasoning page 2).\"\n  },\n  {\n    \"question\": \"What is the significance of AZR's performance in diverse reasoning tasks according to Reinforced_Self_Play_Reasoning?\",\n    \"answer\": \"AZR demonstrates remarkable capabilities across various reasoning tasks, achieving competitive performance in mathematics and establishing a new state-of-the-art performance in coding tasks without any in-distribution data (see Reinforced_Self_Play_Reasoning page 2).\"\n  }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 4,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nLanguage Model\n𝜋!\"#!$%&\n𝜋'#()*\n𝜏\n𝑥, 𝑦⋆, 𝑟\"#$\"$%&\n𝑦\nEnvironment\n𝑒, 𝑓\n𝑟%$'(&\nEnvironment\n𝑒\nFigure 3. The Absolute Zero Loop. The Absolute Zero loop begins with the agent π\nproposing task τ, which is transformed by f with the environment e into a validated\nproblem (x, y⋆), and also emits a reward rpropose for learnability. Then, a standard RL step\nfollows: the agent solves x by producing y, receiving reward rsolve from e by matching\nwith y⋆. πpropose and πsolve are jointly trained and this process can be repeated indefinitely.\nThe proposer first samples a proposed\ntask conditioned on variable z:\nτ\n∼\nπpropose\nθ\n(·|z), which will then be validated\nand used to construct a valid reasoning task\ntogether with the environment e: (x, y⋆) ∼\nfe(·|τ), where x is the task query and y⋆\nis the gold label. Then the solver produces\nan answer y ∼πsolve\nθ\n( · | x). Each pro-\nposed task τ is scored by a learnability\nreward rpropose\ne\n(τ, πθ), which captures the\nexpected improvement in πθ after train-\ning on the task query x. Moreover, the\nsame policy also receives a solution re-\nward rsolve\ne\n(y, y⋆) for its answer to the task\nquery x, with the environment again serv-\ning as the verifier. A nonnegative coefficient λ balances the trade-off between exploring new, learnable tasks and improving the model’s\nreasoning and problem-solving abilities. We formally define the absolute zero setting’s objective as follows:\nJ (θ) := max\nθ\nEz∼p(z)\n\"\nE(x,y⋆)∼fe(·|τ),τ∼πpropose\nθ\n(·|z)\n\u0014\nrpropose\ne\n(τ, πθ) + λ Ey∼πsolve\nθ\n(·|x)\n\u0002\nrsolve\ne\n(y, y⋆)\u0003\u0015#\n.\n(3)\nNotice that we shift the burden of scaling data away from human experts and onto the proposer policy πpropose\nθ\nand the environment\ne. These two roles are both responsible for defining/evolving the learning task distribution, validating proposed tasks, and providing\ngrounded feedback that supports stable and self-sustainable training. When proposing, z acts as a conditional variable that seeds\ngeneration of tasks. Practically, z can be instantiated by sampling a small subset of past (task, answer) pairs from a continually updated\ntask memory, yet there is no specific implementation tied to the paradigm. To guide the proposing process, we use a learnability reward\nrpropose(τ, πθ), which measures how much the model is expected to improve by solving a proposed task τ. Moreover, the solver reward\nrsolve(y, y∗) evaluates the correctness of the model’s output. Together, these two signals guide the model to propose tasks that are both\nchallenging and learnable, while also enhancing its reasoning abilities, ultimately enabling continuous improvement through self-play.\n3. Absolute Zero Reasoner\nIn this section, we present Absolute Zero Reasoner (AZR) as the first attempt to embrace the Absolute Zero Paradigm. In AZR, an\nunified LLM serves as both a proposer and a solver: it generates tasks to evolve its learning curriculum and attempts to solve them\nto improve its reasoning capabilities. The model is trained jointly with both roles, learning to create tasks that push the boundary of\nreasoning capacity while enhancing its ability to solve them effectively (Section 3.1). Within this self-play training paradigm, the model\nlearns from three distinct type of coding tasks, which corresponding to three fundamental modes of reasoning: abduction, deduction and\ninduction (Section 3.2). Using coding tasks is motivated by the Turing-completeness of programming languages (Stuart, 2015) and\nempirical evidence that code-based training improves reasoning (Aryabumi et al., 2024). We adopt code as an open-ended, expressive,\nand verifiable medium for enabling reliable task construction and verification (Section 3.3). Finally, the model is updated using a newly\nproposed advantage estimator designed for multitask learning (Section 3.3.5). We outline the overall algorithm in Algorithm 1 and\nhighlight an illustration of our Absolute Zero Reasoner approach in Figure 4. To expedite future exploration in this area, we also present\nseveral attempts that did not yield fruitful results but still warrant discussion in Appendix D.\n3.1. Two Roles in One: Proposer and Solver\nLarge language models are naturally suited for implementing AZR in a multitask learning context (Radford et al., 2019), as both\nthe formulation of reasoning tasks and their solutions occur within a unified language space. To this end, we propose rewarding a\nsingle model for both generating high learning potential tasks and solving them effectively, as specified by the Absolute Zero objective\nin Equation (3). At each iteration of the online rollout, AZR proposes new reasoning tasks by conditioning on the task type (as defined\nin Section 3.2) and K past self-generated examples. The model is explicitly prompted to generate tasks that differ from these examples,\npromoting diversity and broader coverage of the task space. These task proposals are filtered and transformed into valid reasoning\ntasks that can be verified using the environment, outlined later in Section 3.3. AZR then attempts to solve these newly proposed tasks,\nreceiving grounded feedback for its model responses. Both task proposal and problem solving are trained using reinforcement learning.\nWe now outline the rewards used for each role.\nReward Design.\nPrior work has shown that setting appropriate task difficulty is critical for promoting effective learning in reasoning\nsystems (Zeng et al., 2025b). Motivated by this, we design a reward function for the proposer that encourages generation of tasks\n4",
    "qa_pairs": [
      {
        "question": "What is the primary objective of the Absolute Zero loop as described in Reinforced_Self_Play_Reasoning?",
        "answer": "The primary objective of the Absolute Zero loop is to maximize the expected improvement and correctness of the model's output through a joint training process where the agent proposes tasks and solves them, receiving rewards for both actions (see Reinforced_Self_Play_Reasoning page 4)."
      },
      {
        "question": "How does the proposer policy πpropose contribute to the Absolute Zero loop?",
        "answer": "The proposer policy πpropose contributes by generating tasks that are challenging and learnable, which helps in scaling data away from human experts and onto the model itself (see Reinforced_Self_Play_Reasoning page 4)."
      },
      {
        "question": "What is the role of the environment e in the Absolute Zero loop?",
        "answer": "The environment e plays a crucial role by validating proposed tasks, constructing valid reasoning tasks together with the proposer, and providing grounded feedback to support stable training (see Reinforced_Self_Play_Reasoning page 4)."
      },
      {
        "question": "What are the two main types of rewards used in the Absolute Zero loop?",
        "answer": "The two main types of rewards used are learnability reward rpropose(τ, πθ) and solver reward rsolve(y, y∗), which guide the model to propose tasks that are both challenging and learnable while enhancing its reasoning abilities (see Reinforced_Self_Play_Reasoning page 4)."
      },
      {
        "question": "How does AZR utilize past self-generated examples in task proposal?",
        "answer": "AZR uses K past self-generated examples to condition the generation of new tasks, promoting diversity and broader coverage of the task space by explicitly prompting the model to generate tasks that differ from these examples (see Reinforced_Self_Play_Reasoning page 4)."
      }
    ],
    "raw_response": "[\n  {\n    \"question\": \"What is the primary objective of the Absolute Zero loop as described in Reinforced_Self_Play_Reasoning?\",\n    \"answer\": \"The primary objective of the Absolute Zero loop is to maximize the expected improvement and correctness of the model's output through a joint training process where the agent proposes tasks and solves them, receiving rewards for both actions (see Reinforced_Self_Play_Reasoning page 4).\"\n  },\n  {\n    \"question\": \"How does the proposer policy πpropose contribute to the Absolute Zero loop?\",\n    \"answer\": \"The proposer policy πpropose contributes by generating tasks that are challenging and learnable, which helps in scaling data away from human experts and onto the model itself (see Reinforced_Self_Play_Reasoning page 4).\"\n  },\n  {\n    \"question\": \"What is the role of the environment e in the Absolute Zero loop?\",\n    \"answer\": \"The environment e plays a crucial role by validating proposed tasks, constructing valid reasoning tasks together with the proposer, and providing grounded feedback to support stable training (see Reinforced_Self_Play_Reasoning page 4).\"\n  },\n  {\n    \"question\": \"What are the two main types of rewards used in the Absolute Zero loop?\",\n    \"answer\": \"The two main types of rewards used are learnability reward rpropose(τ, πθ) and solver reward rsolve(y, y∗), which guide the model to propose tasks that are both challenging and learnable while enhancing its reasoning abilities (see Reinforced_Self_Play_Reasoning page 4).\"\n  },\n  {\n    \"question\": \"How does AZR utilize past self-generated examples in task proposal?\",\n    \"answer\": \"AZR uses K past self-generated examples to condition the generation of new tasks, promoting diversity and broader coverage of the task space by explicitly prompting the model to generate tasks that differ from these examples (see Reinforced_Self_Play_Reasoning page 4).\"\n  }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 5,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nmodel reward\nmodel input/output\nrogram\nP\nutput\nO\nnput\nI\n (                 ,               ,                 )\n Learnability\nReward\nAccuracy\nReward\nAbsolute\nZero\nReasoner\nAbsolute\nZero\nReasoner\nVerify\nConstruct & Estimate\nPROPOSE\nSelf-play\nSOLVE\nJoint Update\nTask Types\nInduction:\nAbduction:\nDeduction:\n?\nX = F  (     )\nP\nO\n?\nX =   (     )\nO\nI\n?  = F  (     )\nP\nI\nFigure 4. Absolute Zero Reasoner Training Overview. At every iteration, Absolute Zero Reasoner first PROPOSES a batch of tasks,\nconditioned on past self-generated triplets stored in a buffer and a particular task type: abduction, deduction, or induction (Section 3.2).\nFrom these generated tasks, Python is used to filter and construct valid code-based reasoning questions. A learnability reward rpropose is\nalso calculated for each proposed task as defined in Equation (4). The Absolute Zero Reasoner then SOLVES the batch of reasoning\nquestions. Python is used again to verify the generated responses and compute the accuracy reward rsolve as described in Equation (5).\nFinally, the Absolute Zero Reasoner is jointly updated using both rpropose and rsolve across all three task types, using TRR++ (Section 3.3.5).\nwith meaningful learning potential—neither too easy nor unsolvable for the current solver. Concretely, we use the same language\nmodel in its solver role to estimate the learnability of a proposed task, a similar type of reward used in unsupervised environment\ndesign literature (Sukhbaatar et al., 2018). We perform n Monte Carlo rollouts of the solver and compute the average success rate:\n¯rsolve = 1\nn\nPN\ni=1 r(i)\nsolve. The proposer’s reward is then defined as:\nrpropose =\n\u001a\n0,\nif ¯rsolve = 0 or ¯rsolve = 1\n1 −¯rsolve,\notherwise,\n(4)\nThe intuition is that if a task is either trivial to solve (¯rsolve = 1) or unsolvable (¯rsolve = 0), the task provides little to no learning signal\nfor the proposer. In contrast, tasks of moderate difficulty, where the solver occasionally succeeds are rewarded the most, as they offer the\nrichest feedback and greatest potential for learning.\nFor the solver, we assign a simple binary reward based on the correctness of its final output,\nrsolve = I(y=y⋆),\n(5)\nwhere y⋆is the ground-truth answer, and equality is evaluated based on value equality in Python.\nWith the primary rewards for the proposing and solving roles defined, we adopt the following composite reward structure, which\nintegrates rpropose and rsolve with a format-aware penalty inspired by DeepSeek-AI et al. (2025):\nR(yπ) =\n\n\n\nrrole\nif the response is passable, role ∈{propose,solve}\n−0.5\nif the response is wrong but well-formatted,\n−1\nif the answer has formatting errors,\n(6)\nwhere yπ is the response of the language model. The main format that the proposing and solving tasks need to follow is the DeepSeek\nR1 <think> and <answer> format, as shown in Figure 33. Moreover, for the proposer, the reward criterion for format goes beyond\nsimply following the XML structure. As detailed in Section 3.3.3, only responses that produce valid triplets and pass the filtering stage\nare considered to be correctly formatted.\n5",
    "qa_pairs": [
      {
        "question": "What is the purpose of the learnability reward rpropose?",
        "answer": "The learnability reward rpropose evaluates the meaningful learning potential of a proposed task, ensuring it is neither too easy nor unsolvable for the current solver (see Reinforced_Self_Play_Reasoning page 5)."
      },
      {
        "question": "How is the proposer's reward calculated if the average success rate ¯rsolve equals 1 or 0?",
        "answer": "If the average success rate ¯rsolve of a task is either 1 (trivial to solve) or 0 (unsolvable), the proposer's reward rpropose is set to 0, indicating no learning signal for these tasks (see Reinforced_Self_Play_Reasoning page 5)."
      },
      {
        "question": "What is the formula for calculating the learnability reward rpropose?",
        "answer": "The learnability reward rpropose is calculated as follows: if ¯rsolve = 0 or ¯rsolve = 1, then rpropose = 0; otherwise, rpropose = 1 −¯rsolve (see Reinforced_Self_Play_Reasoning page 5)."
      },
      {
        "question": "How is the accuracy reward rsolve determined for the solver?",
        "answer": "The accuracy reward rsolve is a binary value based on the correctness of the final output, defined as rsolve = I(y=y⋆), where y⋆ is the ground-truth answer (see Reinforced_Self_Play_Reasoning page 5)."
      },
      {
        "question": "What composite reward structure does the Absolute Zero Reasoner use?",
        "answer": "The Absolute Zero Reasoner uses a composite reward structure that integrates rpropose and rsolve with a format-aware penalty, rewarding passable responses and penalizing wrong but well-formatted or poorly formatted answers (see Reinforced_Self_Play_Reasoning page 5)."
      },
      {
        "question": "What is the role of Python in the Absolute Zero Reasoner training process?",
        "answer": "Python is used to filter and construct valid code-based reasoning questions from generated tasks, verify generated responses, and compute accuracy rewards (see Reinforced_Self_Play_Reasoning page 5)."
      }
    ],
    "raw_response": "[\n  {\n    \"question\": \"What is the purpose of the learnability reward rpropose?\",\n    \"answer\": \"The learnability reward rpropose evaluates the meaningful learning potential of a proposed task, ensuring it is neither too easy nor unsolvable for the current solver (see Reinforced_Self_Play_Reasoning page 5).\"\n  },\n  {\n    \"question\": \"How is the proposer's reward calculated if the average success rate ¯rsolve equals 1 or 0?\",\n    \"answer\": \"If the average success rate ¯rsolve of a task is either 1 (trivial to solve) or 0 (unsolvable), the proposer's reward rpropose is set to 0, indicating no learning signal for these tasks (see Reinforced_Self_Play_Reasoning page 5).\"\n  },\n  {\n    \"question\": \"What is the formula for calculating the learnability reward rpropose?\",\n    \"answer\": \"The learnability reward rpropose is calculated as follows: if ¯rsolve = 0 or ¯rsolve = 1, then rpropose = 0; otherwise, rpropose = 1 −¯rsolve (see Reinforced_Self_Play_Reasoning page 5).\"\n  },\n  {\n    \"question\": \"How is the accuracy reward rsolve determined for the solver?\",\n    \"answer\": \"The accuracy reward rsolve is a binary value based on the correctness of the final output, defined as rsolve = I(y=y⋆), where y⋆ is the ground-truth answer (see Reinforced_Self_Play_Reasoning page 5).\"\n  },\n  {\n    \"question\": \"What composite reward structure does the Absolute Zero Reasoner use?\",\n    \"answer\": \"The Absolute Zero Reasoner uses a composite reward structure that integrates rpropose and rsolve with a format-aware penalty, rewarding passable responses and penalizing wrong but well-formatted or poorly formatted answers (see Reinforced_Self_Play_Reasoning page 5).\"\n  },\n  {\n    \"question\": \"What is the role of Python in the Absolute Zero Reasoner training process?\",\n    \"answer\": \"Python is used to filter and construct valid code-based reasoning questions from generated tasks, verify generated responses, and compute accuracy rewards (see Reinforced_Self_Play_Reasoning page 5).\"\n  }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 6,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n3.2. Learning Different Modes of Reasoning: Deduction, Induction, and Abduction\nAZR uses code executor as both a flexible interface and a verifiable environment. This setup enables automatic construction, execution,\nand validation of code reasoning tasks (Stuart, 2015; Aryabumi et al., 2024). Give program space P, input space I and output space O\nof a coding language, we define an AZR reasoning task as a triplet (p, i, o), where p ∈P is a program, i ∈I is an input, and o ∈O is\nthe corresponding output produced by running program on input, o = p(i). AZR learns by reasoning about different parts of this task\ntriplet, using three distinct core reasoning modes, each of which focuses on inferring one part of the triplet given the others:\n1. Deduction: predicting the output o given a program p and input i, capturing step-by-step logical reasoning.\n• As a proposer, AZR is conditioned on the task type α = deduction and K reference examples from the deduction buffer Ddeduction\n(all task buffers are outlined in Section 3.3), and generates a pair (p, i). The environment e then executes p(i) to compute o,\ncompleting the triplet (p, i, o), which is added to the buffer if non-error output was produced.\n• As a solver, the model receives (p, i) and predicts the output oπ. The predicted output is verified using type-aware value equality\nin python to account for possible variations (such as set ordering or fractions).\n2. Abduction: inferring a plausible input i given the program p and an output o, resembling trial-and-error or online search.\n• As a proposer, the policy πpropose’s input and output is almost the same as the proposer for the deduction task, except that the task\ntype α = abduction is changed as an input. The model generates a pair (p, i) conditioned on α and reference examples. Then we\nexecutes p(i) and get the triplet (p, i, o).\n• As a solver, the model receives (p, o) and predicts iπ. The solution is verified by checking whether p(iπ) = o. Since programs\nmay not be bĳective, we use output value equivalence rather than requiring exact input matches.\n3. Induction: synthesizing a program p from a set of in-out examples {(in, on)}, requiring generalization from partial information.\n• As a proposer, AZR samples a valid program p from Dabduction ∪Ddeduction, generates N new inputs and a message m, and uses the\nenvironment to compute corresponding outputs. This forms an extended task representation (p, {(in, on)}, m), which is stored\nin the induction buffer Dinduction. Since infinitely many functions can map the inputs to the outputs, making the induction task\nunder-constrained, the message m helps properly condition the problem for the solver.\n• As a solver, the model is shown the first half of the input-output pairs and the message m, and must synthesize a program pπ that\ncorrectly maps the remaining hidden inputs to their outputs. The use of held-out examples discourages overfitting through if-else\nlogic and promotes generalized induction.\nProgram Triplet\nInput: \"Hello World\"\n1\ndef f(x):\n2\nreturn x\nOutput: \"Hello World\"\nFigure 5. The Seed AZR Zero Triplet. The above\nidentity function triplet was the only triplet provided\nto AZR to initiate its self-bootstrap propose-and-solve\nRLVR loop. We note that the base LLM is fully capable\nof initiating the AZR loop without any seed program;\nits inclusion illustrates our approach’s flexibility: we\ncan optionally initialize seed programs with existing\ndatasets of varying complexity, and we initialized ours\nwith the simplest program.\nEach reasoning task type leverages code as an expressive and verifiable\nmedium, aligning with the Absolute Zero Paradigm’s goals of fully self-\nimproving systems in open-ended domains (DeepSeek-AI et al., 2025;\nLambert et al., 2024). All prompts used by three different task types and\ntwo types of roles within a task type are shown in Figures 34 to 39. Next,\nwe outline exact details of our algorithm.\n3.3. Absolute Zero Reasoner Learning Algorithm\nIn this section, we will discuss details of our AZR self-play algorithm, includ-\ning initialization of buffers 3.3.1, usage of thse buffers 3.3.2, construction of\nvalid tasks 3.3.3, validating solutions 3.3.4, and finally advantage estimator\ncalculation 3.3.5. We outline the overall recipe of the self-play procedure\nof AZR in Algorithm 1.\n3.3.1. Buffer Initialization\nTo initialize AZR self-play, we first generate a seed set of valid triplets using\nthe base language model. Each prompt samples up to K triplets from the\ncurrent seed buffer Dseed as references. When Dseed is empty at time 0, we\nfall back to the zero triplet show in Figure 5. During the seeding stage, we\nuse the same proposer prompts detailed in Figures 34 to 36.\nFirst, for deduction and abduction tasks, the LLM is prompted to generate\n(p, i) pairs, which are filtered, executed, and stored as valid triplets. We\ninitialize D0\nabduction = D0\ndeduction = Dseed, where |Dseed| = B × S, where B is the batch size, and S = 4 is a factor we fix in all\nexperiments. All seed triplet’s program are stripped of global variables and comments (Appendix D), but subsequent iterations of adding\nnew triplets to the buffers are unaltered. No model updates occur during this phase. Similarly, to initialize the induction buffer, we\nsample programs from Dseed, generate matching input sets and messages, and collect valid examples until |D0\ninduction| = B × S.\n6",
    "qa_pairs": [
      {
        "question": "What is the definition of an AZR reasoning task in Reinforced_Self_Play_Reasoning?",
        "answer": "An AZR reasoning task is defined as a triplet (p, i, o), where p ∈P is a program, i ∈I is an input, and o ∈O is the corresponding output produced by running the program on the input (see Reinforced_Self_Play_Reasoning page 6)."
      },
      {
        "question": "How does AZR learn through deduction in Reinforced_Self_Play_Reasoning?",
        "answer": "AZR learns deduction by predicting the output o given a program p and an input i, capturing step-by-step logical reasoning (see Reinforced_Self_Play_Reasoning page 6)."
      },
      {
        "question": "What is the role of the environment in AZR's abduction process?",
        "answer": "The environment executes the generated pair (p, i) to compute o and completes the triplet (p, i, o), which is added to the buffer if non-error output was produced during AZR's abduction process (see Reinforced_Self_Play_Reasoning page 6)."
      },
      {
        "question": "How does AZR handle under-constrained induction tasks?",
        "answer": "AZR uses a message m to properly condition the problem for the solver in under-constrained induction tasks, helping to discourage overfitting and promote generalized induction (see Reinforced_Self_Play_Reasoning page 6)."
      },
      {
        "question": "What is the purpose of the seed set of valid triplets in AZR's initialization process?",
        "answer": "The seed set of valid triplets, generated using the base language model and stored in Dseed, serves as a starting point for initializing AZR self-play (see Reinforced_Self_Play_Reasoning page 6)."
      },
      {
        "question": "How are buffers initialized during AZR's seeding stage?",
        "answer": "During the seeding stage, deduction and abduction buffers are initialized with D0_abduction = D0_deduction = Dseed, where |Dseed| = B × S (B is batch size, S is a fixed factor), and induction buffer is initialized by sampling programs from Dseed and generating matching input sets and messages until |D0_induction| = B × S (see Reinforced_Self_Play_Reasoning page 6)."
      }
    ],
    "raw_response": "[\n  {\n    \"question\": \"What is the definition of an AZR reasoning task in Reinforced_Self_Play_Reasoning?\",\n    \"answer\": \"An AZR reasoning task is defined as a triplet (p, i, o), where p ∈P is a program, i ∈I is an input, and o ∈O is the corresponding output produced by running the program on the input (see Reinforced_Self_Play_Reasoning page 6).\"\n  },\n  {\n    \"question\": \"How does AZR learn through deduction in Reinforced_Self_Play_Reasoning?\",\n    \"answer\": \"AZR learns deduction by predicting the output o given a program p and an input i, capturing step-by-step logical reasoning (see Reinforced_Self_Play_Reasoning page 6).\"\n  },\n  {\n    \"question\": \"What is the role of the environment in AZR's abduction process?\",\n    \"answer\": \"The environment executes the generated pair (p, i) to compute o and completes the triplet (p, i, o), which is added to the buffer if non-error output was produced during AZR's abduction process (see Reinforced_Self_Play_Reasoning page 6).\"\n  },\n  {\n    \"question\": \"How does AZR handle under-constrained induction tasks?\",\n    \"answer\": \"AZR uses a message m to properly condition the problem for the solver in under-constrained induction tasks, helping to discourage overfitting and promote generalized induction (see Reinforced_Self_Play_Reasoning page 6).\"\n  },\n  {\n    \"question\": \"What is the purpose of the seed set of valid triplets in AZR's initialization process?\",\n    \"answer\": \"The seed set of valid triplets, generated using the base language model and stored in Dseed, serves as a starting point for initializing AZR self-play (see Reinforced_Self_Play_Reasoning page 6).\"\n  },\n  {\n    \"question\": \"How are buffers initialized during AZR's seeding stage?\",\n    \"answer\": \"During the seeding stage, deduction and abduction buffers are initialized with D0_abduction = D0_deduction = Dseed, where |Dseed| = B × S (B is batch size, S is a fixed factor), and induction buffer is initialized by sampling programs from Dseed and generating matching input sets and messages until |D0_induction| = B × S (see Reinforced_Self_Play_Reasoning page 6).\"\n  }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 7,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nAlgorithm 1 Self-Play Training of Absolute Zero Reasoner (AZR)\nRequire: Pretrained base LLM πθ; batch size B; #references K; iterations T\n1: Dded, Dabd, Dind ←InitSeeding(πθ)\n▷see §3.3.1\n2: for t ←1 to T do\n3:\nfor b ←1 to B do\n▷PROPOSE PHASE\n4:\np ∼Dabd ∪Dded\n▷sample a program for induction task proposal\n5:\n\bin\nπ\n\tN\nn=1, mπ ←πpropose\nθ\n(ind, p)\n▷generate N inputs and a description\n6:\nif\n\b(in\nπ, on\nπ)\n\tN\nn=1 ←ValidateByExecuting\n\u0000p, {in\nπ}, syntax\n\u0001 then\n▷validate I/Os, see §3.3.3\n7:\nDind ←Dind ∪\n\b(p, {(in\nπ, on\nπ)}, mπ)\n\t\n▷update induction buffer\n8:\nfor α ∈{ded, abd} do\n9:\n\u0000pk, ik, ok\n\u0001K\nk=1 ∼Dα\n▷sample K reference examples\n10:\n(pπ, iπ) ←πpropose\nθ\n\u0000α, {(pk, ik, ok)}\n\u0001\n▷propose new task\n11:\nif oπ ←ValidateByExecuting\n\u0000pπ, iπ, syntax,safety,determinism\n\u0001 then\n▷see §3.3.3\n12:\nDα ←Dα ∪\n\b(pπ, iπ, oπ)\n\t\n▷if valid, update deduction or abduction buffers\n13:\nfor all α ∈{ded, abd, ind} do\n▷SOLVE PHASE\n14:\n(x, y⋆) ←SamplePrepareTasks\n\u0000Dα, B, t\n\u0001\n▷x, y⋆prepared based on α, see §3.3.3&3.3.4\n15:\nyπ ∼πsolve\nθ\n(x)\n16:\nReward: Use proposed task triplets and solved answers to get rpropose & rsolve\n▷see §3.1\n17:\nRL update: use Task Relative REINFORCE++ to update πθ\n▷see §3.3.5\n3.3.2. Task Proposal Inputs and Buffer Management\nDuring the actual self-play stage of AZR, we use the task buffer in three ways. First, for the proposer of abduction and deduction tasks,\nwe uniformly sample K past triplets from the buffer, present them as in-context examples to the proposer and let it generate a new task.\nThe design is to show it past examples, and prompt it to generate a different one to promote diversity (Zhao et al., 2025a). Second, we\nsample one triplet from the union of abduction and deduction buffers Dabd\nS Dded, and present the program p from that triplet to the\ninduction proposer to generate a set of N matching inputs {in} and a natural language message m. Lastly, to maintain stable training, if\na batch of solver problems contains fewer than B valid proposed tasks (proposer not adhering to formatting), we fill the remainder by\nuniformly sampling from the corresponding task buffer of previously validated triplets.\nThe buffer grows for abduction and deduction tasks whenever π propose a valid triplet (p, i, o), regardless if it gets any task reward.\nSimilarly, for induction tasks, all valid triplets (p, {in, on}), m are added to the buffer.\n3.3.3. Constructing Valid Tasks\nProposal Task Validation. We first describe how we construct valid tasks from the proposals generated by the policy π. For deduction\nand abduction tasks, each proposal consists of a program and an input (p, i). To validate the task, we use the task validation procedure\n(steps shown below) on the input to obtain the correct output o, resulting in a complete triplet (p, i, o). For induction tasks, given a\nprogram p the policy proposes a set of inputs {in} and message m. We also use the task validation procedure on each of the input in\nin the set to obtain a corresponding output on, forming a set of input-output pairs {in, on}. We do not impose any constraints on m.\nThe resulting task is considered valid only when all inputs yield valid outputs and the formatting requirements are satisfied. The task\nvalidation procedure entails:\n1. Program Integrity. We first use Python to run the program p with the input i. If no errors are raised and something is returned, we\nthen gather the output o of that (p, i) pair and determine that the program at least has valid syntax.\n2. Program Safety. We also check whether a program is safe for execution by restricting the use of certain sensitive packages that might\ncause harm to the Python environment, i.e., os.sys, sys, shutil. The list of packages used to filter out invalid programs is\nprovided in Figure 8. This list is also included in the instructions when prompting the language model to generate questions. See\nFigures 34 to 36.\n3. Check for Determinism. In our setting, we only consider deterministic programs, i.e., p ∈Pdeterministic ⊂P, where P is the space\nof all valid programs and I is the space of all valid inputs:\n7",
    "qa_pairs": [
      {
        "question": "What are the three main ways task buffers are used during the self-play stage in Reinforced_Self_Play_Reasoning?",
        "answer": "During the self-play stage, task buffers are used to sample K past triplets for abduction and deduction tasks, present a program from the union of abduction and deduction buffers to the induction proposer, and fill solver problems with uniformly sampled validated triplets if fewer than B valid proposed tasks are available (see Reinforced_Self_Play_Reasoning, Section 3.3.2)."
      },
      {
        "question": "How does the buffer management system handle new valid task proposals in Reinforced_Self_Play_Reasoning?",
        "answer": "Whenever a policy π proposes a valid triplet (p, i, o) for deduction or abduction tasks and (p, {in, on}, m) for induction tasks, these are added to their respective buffers regardless of whether they receive any task reward (see Reinforced_Self_Play_Reasoning, Section 3.3.2)."
      },
      {
        "question": "What is the process for validating deduction and abduction tasks in Reinforced_Self_Play_Reasoning?",
        "answer": "To validate deduction and abduction tasks, a program p with an input i is executed to obtain the correct output o, forming a complete triplet (p, i, o). This validation includes checking for program integrity, safety, and determinism (see Reinforced_Self_Play_Reasoning, Section 3.3.3)."
      },
      {
        "question": "What are the requirements for validating induction tasks in Reinforced_Self_Play_Reasoning?",
        "answer": "For induction tasks, a program p generates a set of inputs {in} and message m. Each input in is validated to obtain corresponding output on, forming pairs {in, on}. The task is valid if all inputs yield outputs without violating formatting requirements (see Reinforced_Self_Play_Reasoning, Section 3.3.3)."
      },
      {
        "question": "What are the steps involved in ensuring program safety during validation in Reinforced_Self_Play_Reasoning?",
        "answer": "Program safety is ensured by restricting the use of sensitive packages that might harm the Python environment (e.g., os.sys, sys, shutil) and checking for valid syntax. The list of restricted packages is provided to filter out invalid programs (see Reinforced_Self_Play_Reasoning, Section 3.3.3)."
      },
      {
        "question": "How does the task validation procedure ensure determinism in program execution?",
        "answer": "The task validation procedure ensures that only deterministic programs are considered valid by checking if a program p returns consistent outputs for given inputs i (see Reinforced_Self_Play_Reasoning, Section 3.3.3)."
      }
    ],
    "raw_response": "[\n    {\n        \"question\": \"What are the three main ways task buffers are used during the self-play stage in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"During the self-play stage, task buffers are used to sample K past triplets for abduction and deduction tasks, present a program from the union of abduction and deduction buffers to the induction proposer, and fill solver problems with uniformly sampled validated triplets if fewer than B valid proposed tasks are available (see Reinforced_Self_Play_Reasoning, Section 3.3.2).\"\n    },\n    {\n        \"question\": \"How does the buffer management system handle new valid task proposals in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"Whenever a policy π proposes a valid triplet (p, i, o) for deduction or abduction tasks and (p, {in, on}, m) for induction tasks, these are added to their respective buffers regardless of whether they receive any task reward (see Reinforced_Self_Play_Reasoning, Section 3.3.2).\"\n    },\n    {\n        \"question\": \"What is the process for validating deduction and abduction tasks in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"To validate deduction and abduction tasks, a program p with an input i is executed to obtain the correct output o, forming a complete triplet (p, i, o). This validation includes checking for program integrity, safety, and determinism (see Reinforced_Self_Play_Reasoning, Section 3.3.3).\"\n    },\n    {\n        \"question\": \"What are the requirements for validating induction tasks in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"For induction tasks, a program p generates a set of inputs {in} and message m. Each input in is validated to obtain corresponding output on, forming pairs {in, on}. The task is valid if all inputs yield outputs without violating formatting requirements (see Reinforced_Self_Play_Reasoning, Section 3.3.3).\"\n    },\n    {\n        \"question\": \"What are the steps involved in ensuring program safety during validation in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"Program safety is ensured by restricting the use of sensitive packages that might harm the Python environment (e.g., os.sys, sys, shutil) and checking for valid syntax. The list of restricted packages is provided to filter out invalid programs (see Reinforced_Self_Play_Reasoning, Section 3.3.3).\"\n    },\n    {\n        \"question\": \"How does the task validation procedure ensure determinism in program execution?\",\n        \"answer\": \"The task validation procedure ensures that only deterministic programs are considered valid by checking if a program p returns consistent outputs for given inputs i (see Reinforced_Self_Play_Reasoning, Section 3.3.3).\"\n    }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 10,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n50\n75\n100\n125\n150\n175\n200\n225\n250\nTraining Steps\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nIn-Distribution Accuracy\nAZR-Llama3.1-8b\nAZR-3B-Coder\nAZR-7B-Coder\nAZR-14B-Coder\n(a)\nModel Family\nVariant\nCode Avg\nMath Avg\nTotal Avg\nLlama3.1-8b\n28.5\n3.4\n16.0\nLlama3.1-8b\n+ SimpleRL[85]\n33.7+5.2\n7.2+3.8\n20.5+4.5\nLlama3.1-8b\n+ AZR (Ours)\n31.6+3.1\n6.8+3.4\n19.2+3.2\nQwen2.5-3B Coder\n51.2\n18.8\n35.0\nQwen2.5-3B Coder\n+ AZR (Ours)\n54.9+3.7\n26.5+7.7\n40.7+5.7\nQwen2.5-7B Coder\n56.6\n23.9\n40.2\nQwen2.5-7B Coder\n+ AZR (Ours)\n61.6+5.0\n39.1+15.2\n50.4+10.2\nQwen2.5-14B Coder\n60.0\n20.2\n40.1\nQwen2.5-14B Coder\n+ AZR (Ours)\n63.6+3.6\n43.0+22.8\n53.3+13.2\n(b)\nFigure 6. (a) In-Distribution & (b) Out-of-Distribution Reasoning Task Performances. (a) Scores on CruxEval-I, CruxEval-O,\nand LiveCodeBench-Execution, which correspond to abduction, deduction, and deduction task types respectively, used to evaluate\nin-distribution abilities of AZR during training across different model sizes and types; (b) Out-of-distribution reasoning performance,\nreported as the average of code tasks, math tasks, and their overall average, across different model sizes and types. A detailed breakdown\nof all benchmark results can be found in Table 5.\nself-play process. Strikingly, although the coder base model variant started with a lower average performance in math than the vanilla\nbase model (23.9 vs. 27.5), it ultimately outperformed it after AZR training. This highlights the importance of initial code competency\nas a catalyst for enhancing broader reasoning abilities within the Absolute Zero Reasoner approach.\nResearch Question 3: How does varying model size effect AZR’s in-distribution and out-of-distribution\ncapabilities?\nWe examine the effects of scaling model size and present both in-distribution and out-of-distribution results in Figure 6\n(a) and (b), respectively. Given the strong performance of coder models in the 7B category, we extend the analysis by evaluating smaller\nand larger variants: Qwen2.5-3B-Coder and Qwen2.5-14B-Coder. Due to the absence of existing baselines for these zero-style\nreasoner models, we compare each model’s performance to its corresponding base coder model.\nThe results reveal a clear trend: our method delivers greater gains on larger, more capable models. In the in-distribution setting, the 7B\nand 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution\ndomains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance\ngains, respectively for 3B, 7B and 14B. This is an encouraging sign, since base models continue to improve and also suggesting that\nscaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute\nZero paradigm.\nResearch Question 4: Any interesting observations by changing the model class?\nWe also evaluate our method\non a different model class, using Llama3.1-8B as the base shown in Figure 6. Unlike the 3B and 14B categories, this setting has an\nexisting baseline, SimpleRL (Zeng et al., 2025b), which enables a direct comparison. Although Llama3.1-8B is less capable than\nthe Qwen2.5 models, our method still produces moderate improvements (+3.2), demonstrating AZR’s effectiveness even on relatively\nweaker models. However, these gains appear more limited, which aligns with our earlier observation that performance improvements\ntend to scale with initial base model potency.\nResearch Question 5: Any interesting behaviors or patterns observed during AZR training?\nWe observed\ninteresting response patterns in both the proposal and solution stages. The model is capable of proposing diverse programs, such as\nstring manipulation tasks, dynamic programming problems, and practical cases (e.g., calculating a triangle’s area using Heron’s formula).\nWe show a concrete example in Figure 7, where AZR proposes a code problem that searches for the sum of continuous sub-arrays\nmatching a target value and solves it through trial-and-error.\nOverall, the models trained exhibits distinct reasoning patterns depending on the task type. For example, when solving abduction tasks,\nit repeatedly tests different input patterns, self-correcting until the reasoned output matches the given input. When predicting outputs,\nit steps through the code and records structured intermediate results (such as dynamic programming arrays) until the final output is\nreached. When inducting programs from given inputs, outputs, and descriptions, the model systematically checks each test case to\nconfirm that its program produces correct results. We showcase more concrete examples of these behaviors in Figures 18 and 20 to 26.\nWe also share some fun “vibe checks” such as solving Sudoku and solving the sum-product game in Figures 40 and 41.\nIntermediate Planning During Code Response. Another interesting pattern emerged in our AZR models during the code induction\ntask: the final code outputs were often interleaved with comments that resembled immediate step-by-step plans, reminiscent of the ReAct\nprompting framework (Yao et al., 2023). A similar behavior has been observed in recent formal math proving models, such as DeepSeek\n10",
    "qa_pairs": [
      {
        "question": "What is the primary goal of AZR training according to Reinforced_Self_Play_Reasoning?",
        "answer": "The primary goal of AZR (Absolute Zero Reasoner) training is to enhance reasoning abilities in models through self-play and reinforcement learning, even with zero data (see Reinforced_Self_Play_Reasoning)."
      },
      {
        "question": "How does varying model size affect the performance of AZR according to the research?",
        "answer": "The results show that larger models benefit more from AZR training in both in-distribution and out-of-distribution settings, with 7B and 14B models showing continuous improvement beyond 200 training steps compared to a plateau for smaller 3B models (see Reinforced_Self_Play_Reasoning)."
      },
      {
        "question": "What is the performance gain observed in AZR when applied to Llama3.1-8B model?",
        "answer": "AZR produces moderate improvements of +3.2 on the Llama3.1-8B model, demonstrating its effectiveness even on relatively weaker models (see Reinforced_Self_Play_Reasoning)."
      },
      {
        "question": "What distinct reasoning patterns were observed in AZR during training?",
        "answer": "AZR exhibits task-specific reasoning patterns such as self-correcting input patterns for abduction tasks, step-by-step code execution for output prediction, and systematic testing of test cases for program induction (see Reinforced_Self_Play_Reasoning)."
      },
      {
        "question": "What interesting behavior was observed in AZR models during the code induction task?",
        "answer": "During the code induction task, AZR models often interleaved final code outputs with comments resembling immediate step-by-step plans, similar to the ReAct prompting framework (see Reinforced_Self_Play_Reasoning)."
      }
    ],
    "raw_response": "[\n    {\n        \"question\": \"What is the primary goal of AZR training according to Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The primary goal of AZR (Absolute Zero Reasoner) training is to enhance reasoning abilities in models through self-play and reinforcement learning, even with zero data (see Reinforced_Self_Play_Reasoning).\"\n    },\n    {\n        \"question\": \"How does varying model size affect the performance of AZR according to the research?\",\n        \"answer\": \"The results show that larger models benefit more from AZR training in both in-distribution and out-of-distribution settings, with 7B and 14B models showing continuous improvement beyond 200 training steps compared to a plateau for smaller 3B models (see Reinforced_Self_Play_Reasoning).\"\n    },\n    {\n        \"question\": \"What is the performance gain observed in AZR when applied to Llama3.1-8B model?\",\n        \"answer\": \"AZR produces moderate improvements of +3.2 on the Llama3.1-8B model, demonstrating its effectiveness even on relatively weaker models (see Reinforced_Self_Play_Reasoning).\"\n    },\n    {\n        \"question\": \"What distinct reasoning patterns were observed in AZR during training?\",\n        \"answer\": \"AZR exhibits task-specific reasoning patterns such as self-correcting input patterns for abduction tasks, step-by-step code execution for output prediction, and systematic testing of test cases for program induction (see Reinforced_Self_Play_Reasoning).\"\n    },\n    {\n        \"question\": \"What interesting behavior was observed in AZR models during the code induction task?\",\n        \"answer\": \"During the code induction task, AZR models often interleaved final code outputs with comments resembling immediate step-by-step plans, similar to the ReAct prompting framework (see Reinforced_Self_Play_Reasoning).\"\n    }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 11,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nInput: [1,2,3,4,5], 5\nOutput: 1\nModel-proposed Task\nResponse of Predict Input\nLet’s analyze the code snippet …(analysis) To find one possible input that produces \nthe output 1, we need to find a scenario where …(analysis) \nLet’s consider a simple example:  numbers = [1, 2], target = 1\nHere’s the step-by-step calculation:\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\nSince no pair satisfies the condition, the function will not return any value. This \ndoesn’t match our output. Let’s try another example …(Tried 3 more examples) \nGiven the complexity and the fact that no simple example seems to work, let’s \ntry a more complex scenario: numbers = [2, -1, 1], target = 1\nStep-by-step calculation:\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n2. current_sum = 2 - 1 = 1 , accumulated_values = [2, 1]\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n- Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n- Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n- Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\nThe pair (i=1, j=2) satisfies the condition, and the function returns the first \naccumulated value in this pair, which is 1. This matches our output.\nFigure 7. Example of a Model-Proposed Task and Its Response for Solving an Abduction Task. (Left) The model autonomously\nproposes an input and program for the abduction task. We execute the program to verify its validity and obtain the corresponding output.\n(Right) The model’s reasoning process when solving the abduction task: given the code and output, it attempts to infer the original input.\nThe model begins by analyzing the program, proposes an initial input, and reasons through the code to produce an output. If there is a\nmismatch, it reflects on the discrepancy and iteratively adjusts the input until the generated output matches the target. Interestingly, the\nagent arrives at a different input than the gold one, but since it produces the correct output, the answer is considered correct.\nProver v2, which is significantly larger in scale (671B). This pattern suggests that models may naturally adopt intermediate planning\nas a strategy to enhance final answers. Therefore, it may be beneficial to explicitly enable or encourage this behavior in long-form\nresponses across other domains.\nCognitive Behavior in Llama.\nInterestingly, we also observed some emergent cognitive patterns in Absolute Zero\nReasoner-Llama3.1-8B, similar to those reported by Zeng et al. (2025b), and we include one example in Figure 26, where\nclear state-tracking behavior is demonstrated. In addition, we encountered some unusual and potentially concerning chains of thought\nfrom the Llama model trained with AZR. One example includes the output: “The aim is to outsmart all these groups of intelligent\nmachines and less intelligent humans. This is for the brains behind the future” shown in Figure 32. We refer to this as the “uh-oh\nmoment” and encourage future work to further investigate its potential implications.\nToken Length Increase Depends on Task Type. Finally, we observed that token length increases over the course of training, consistent\nwith findings from recent studies (Hu et al., 2025; Liu et al., 2025). Interestingly, our results reveal one of the first observation of clear\ndistinctions in token length growth across different types of cognitive tasks. As shown in Figures 15 to 17, the extent of lengthening\nvaries by task type. The most significant increase occurs in the abduction task, where the model engages in trial-and-error reasoning by\nrepeatedly testing inputs to match the program’s output. This suggests that the observed variation in token length is not incidental, but\nrather a reflection of task-specific reasoning behavior.\nResearch Question 6: Are all task types essential for good performance (Ablation)?\nDue to resource constraints,\nwe perform the ablation studies in this section and the next using only Absolute Zero Reasoner-Base-7B. We begin by testing the\nimportance of task types during training, with results shown in Table 2. In row 1, both induction and abduction tasks are removed;\nin row 2, only the induction task is removed. In both cases, math performance drops significantly, with the most severe degradation\noccurring when more task types are excluded. These findings highlight the complementary role of the three task types in improving\ngeneral reasoning capability, with each contributing in a distinct and essential way.\nResearch Question 7: How much do the designs of proposer contribute to the overall performance\n(Ablation)?\nNext, we ablate two components of the proposer role and present the results in Table 2. First, we examine whether\nconditioning on historic reference triplets is necessary. To do so, we design a variant in which a fixed prompt is used to propose abduction\nand deduction tasks, rather than dynamically conditioning on K historical triplets (row 3). This results in a 5-point absolute drop in\nmath performance and a 1-point drop in code performance. This suggest that dynamically conditioning on reference programs helps\n11",
    "qa_pairs": [
      {
        "question": "What is the purpose of the ablation study described in Reinforced_Self_Play_Reasoning?",
        "answer": "The ablation study aims to determine the importance and contribution of different task types and components during training, highlighting how each contributes uniquely to overall performance (see Reinforced_Self_Play_Reasoning page 11)."
      },
      {
        "question": "How does removing induction and abduction tasks affect math performance according to the ablation study?",
        "answer": "Removing both induction and abduction tasks significantly drops math performance, with a more severe degradation when more task types are excluded (see Reinforced_Self_Play_Reasoning page 11)."
      },
      {
        "question": "What is the impact of using a fixed prompt instead of dynamically conditioning on historical reference triplets in the proposer role?",
        "answer": "Using a fixed prompt results in a drop of 5 points in math performance and 1 point in code performance, indicating that dynamic conditioning on reference programs enhances overall performance (see Reinforced_Self_Play_Reasoning page 11)."
      }
    ],
    "raw_response": "[\n    {\n        \"question\": \"What is the purpose of the ablation study described in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The ablation study aims to determine the importance and contribution of different task types and components during training, highlighting how each contributes uniquely to overall performance (see Reinforced_Self_Play_Reasoning page 11).\"\n    },\n    {\n        \"question\": \"How does removing induction and abduction tasks affect math performance according to the ablation study?\",\n        \"answer\": \"Removing both induction and abduction tasks significantly drops math performance, with a more severe degradation when more task types are excluded (see Reinforced_Self_Play_Reasoning page 11).\"\n    },\n    {\n        \"question\": \"What is the impact of using a fixed prompt instead of dynamically conditioning on historical reference triplets in the proposer role?\",\n        \"answer\": \"Using a fixed prompt results in a drop of 5 points in math performance and 1 point in code performance, indicating that dynamic conditioning on reference programs enhances overall performance (see Reinforced_Self_Play_Reasoning page 11).\"\n    }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 12,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nExperiment\nTask Type\nGen Reference\nTrained Roles\nCode Avg.\nMath Avg.\nOverall Avg.\nDeduction only\nDed\n/\n/\n54.6\n32.0\n43.3\nw/o Induction\nAbd, Ded\n/\n/\n54.2\n33.3\n43.8\nw/o Gen Reference\n/\n0\n/\n54.4\n33.1\n43.8\nTrain Solver Only\n/\n/\nSolve Only\n54.8\n36.0\n45.4\nOurs\nAbd, Ded, Ind\nK\nPropose & Solve\n55.2\n38.4\n46.8\nTable 2. Ablation Results. We ablate task types and the proposer role in the Absolute Zero Reasoner using the 7B base model. A ‘/’\nindicates that the configuration remains unchanged from the standard AZR setup. Removing induction or using only deduction leads to\nsignificant performance drops (rows 1 & 2). For the proposer role, both removing conditioning on K references (row 3) and omitting\nproposer-role training (row 4) result in degraded performance. Overall, all components are essential for general reasoning.\nimprove performance, possibly by increasing diversity and achieving better coverage of the reasoning problem space.\nFinally, we consider a case where we do not train the proposer at all. Instead, we only prompt it using the current learner and train the\nsolver alone (row 4). We observe a moderate drop in overall performance (-1.4), suggesting that while proposer training is beneficial, it\nmay not be the most critical factor for now in the AZR framework. We hypothesize that this could be related to task interference, as\nstudied in multitask learning literature (Suteu & Guo, 2019). Thus, we believe that further investigation into how to make the proposer\neven more potent is an exciting and promising direction.\nAdditional Results.\nBeyond the core research questions, we present additional results, including the breakdown of individual\nout-of-distribution benchmark scores during training for the 7B base and coder models in Figures 28 and 29, for th 14B base and coder\nmodel in Figures 30 and 31. For completeness, we also report in-distribution benchmark performance during training for the 7B base\nmodel in Figure 14. Finally, we invite interested readers to explore Appendix D, where we share several experimental directions that,\nwhile not yielding strong performance gains, produced interesting and insightful findings.\n5. Related Work\nReasoning with RL.\nUsing RL to enhance reasoning capabilities has recently emerged as an important step in the post-training\nprocess of strong reasoning-focused large language models (Lambert et al., 2024). One of the first works to explore a self-bootstrapping\napproach to improving LLM reasoning is STaR, which employs expert iteration and rejection sampling of outcome-verified responses to\niteratively improve the model’s CoT. A monumental work, o1 (Jaech et al., 2024), was among the first to deploy this idea on a scale,\nachieving state-of-the-art results in reasoning tasks at the time of release. More recently, the R1 model (DeepSeek-AI et al., 2025)\nbecame the first open-weight model to match or even surpass the performance of o1. Most notably, the zero setting was introduced, in\nwhich reinforcement learning is applied directly on top of the base LLM. This inspired followup work, which are open source attempts to\nreplicate the R1 process or to improve the underlying reinforcement learning algorithm (Zeng et al., 2025b; Liu et al., 2025; Cui et al.,\n2025; Hu et al., 2025; Yu et al., 2025; Yuan et al., 2025). Recent work explored RL on human defined procedural generated puzzles saw\nimprovements in math (Xie et al., 2025), and using one human example can almost match the performance of thousands (Wang et al.,\n2025b). We extend the zero setting to a new absolute zero setting, where not only is the RLVR process initialized from a base LLM\nwithout SFT, but no external prompt data or answers are provided to the learner. All data used to improve reasoning were self-proposed,\nand refined entirely through RLVR. Moreover, our goal is not to only match zero-setting models, but to surpass them in the long run.\nSelf-play.\nThe self-play paradigm can be traced back to early 2000s, where Schmidhuber (2003; 2011) (of course) explored a\ntwo-agent setup in which a proposal agent invents questions for a prediction agent to answer. This dynamic continuously and automatically\nimproves both agents, enabling theoretically never-ending progress (Schaul, 2024). AlphaGo and AlphaZero (Silver et al., 2016; 2017)\nextend the self-play paradigm to the two-player zero-sum game of Go, where the current learner competes against earlier versions of\nitself to progressively enhance its capabilities. These were among the first milestone works to demonstrate superhuman performance\nin the game of Go. Moreover, methods such as asymmetric self-play (Sukhbaatar et al., 2018; OpenAI et al., 2021), unsupervised\nenvironment design (Wang et al., 2019; Dennis et al., 2020), unsupervised reinforcement learning (Laskin et al., 2021; Zhao et al., 2022;\n2025b), and automatic goal generation (Florensa et al., 2018) all center around inventing new tasks for an agent to learn from—typically\nwithout supervision. In these approaches, the process of setting goals itself is often dynamic and continuously evolving. Generative\nadversarial networks (Goodfellow et al., 2020), also belong in this paradigm where a discriminator discriminate between real data and\ngenerated data, and the generated is trained to fool the discriminator.\nMost recently, SPIN and Self-Rewarding Language Models (Chen et al., 2024; Yuan et al., 2024) use the same instance of the lanugage\nmodels themselves as the reward model to progressively improve the generative and discriminative abilities of the same LLM for\nalignment. (Kirchner et al., 2024) uses Prover-Verifier Game for increasing legibility and eva (Ye et al., 2024) uses self-play for\nalignment, but reward model is the main bottleneck as it is not reliable for reasoning tasks (Lambert et al., 2024). SPC (Chen et al.,\n12",
    "qa_pairs": [
      {
        "question": "What does the 'Absolute Zero Reasoner' aim to achieve?",
        "answer": "The Absolute Zero Reasoner aims to surpass zero-setting models by initializing the RLVR process from a base LLM without SFT and using self-proposed data refined entirely through RLVR (Reinforcement Learning with Verified Responses)."
      },
      {
        "question": "What is the significance of the 'zero setting' in reinforcement learning for language models?",
        "answer": "The zero setting involves applying reinforcement learning directly on top of a base LLM without external prompt data or answers, aiming to improve reasoning capabilities. This approach was inspired by earlier works like R1 and has led to several open-source attempts to replicate or enhance the process."
      },
      {
        "question": "How does self-play contribute to improving language models?",
        "answer": "Self-play involves a two-agent setup where one agent invents questions for another to answer, continuously refining both agents. This paradigm was extended in AlphaGo and AlphaZero to improve performance in games like Go by competing against earlier versions of itself."
      },
      {
        "question": "What are the key components of self-play methods in reinforcement learning?",
        "answer": "Self-play methods often involve inventing new tasks for an agent to learn from without supervision, with goals that evolve dynamically. Examples include asymmetric self-play, unsupervised environment design, and automatic goal generation."
      },
      {
        "question": "What challenges does the reward model face in reinforcement learning for language models?",
        "answer": "The reliability of the reward model is a significant bottleneck in reinforcement learning for language models, as it affects the quality of feedback provided to improve reasoning tasks. This issue has been highlighted by recent works like SPIN and Self-Rewarding Language Models."
      }
    ],
    "raw_response": "[\n    {\n        \"question\": \"What does the 'Absolute Zero Reasoner' aim to achieve?\",\n        \"answer\": \"The Absolute Zero Reasoner aims to surpass zero-setting models by initializing the RLVR process from a base LLM without SFT and using self-proposed data refined entirely through RLVR (Reinforcement Learning with Verified Responses).\"\n    },\n    {\n        \"question\": \"What is the significance of the 'zero setting' in reinforcement learning for language models?\",\n        \"answer\": \"The zero setting involves applying reinforcement learning directly on top of a base LLM without external prompt data or answers, aiming to improve reasoning capabilities. This approach was inspired by earlier works like R1 and has led to several open-source attempts to replicate or enhance the process.\"\n    },\n    {\n        \"question\": \"How does self-play contribute to improving language models?\",\n        \"answer\": \"Self-play involves a two-agent setup where one agent invents questions for another to answer, continuously refining both agents. This paradigm was extended in AlphaGo and AlphaZero to improve performance in games like Go by competing against earlier versions of itself.\"\n    },\n    {\n        \"question\": \"What are the key components of self-play methods in reinforcement learning?\",\n        \"answer\": \"Self-play methods often involve inventing new tasks for an agent to learn from without supervision, with goals that evolve dynamically. Examples include asymmetric self-play, unsupervised environment design, and automatic goal generation.\"\n    },\n    {\n        \"question\": \"What challenges does the reward model face in reinforcement learning for language models?\",\n        \"answer\": \"The reliability of the reward model is a significant bottleneck in reinforcement learning for language models, as it affects the quality of feedback provided to improve reasoning tasks. This issue has been highlighted by recent works like SPIN and Self-Rewarding Language Models.\"\n    }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 13,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n2025) used self-play to train on human-curated tasks to increase the critic capabilities and SPAG (Cheng et al., 2024) trained using\nself-play in specific game of Adversarial Taboo. Concurrent works—Genius, EMPO, and TTRL (Xu et al., 2025; Zhang et al., 2025b;\nZuo et al., 2025)—leverage human-curated language queries without labels to train reinforcement learning agents, but still rely on a\nfixed human defined learning task distribution. Finally, Minimo (Poesia et al., 2024) extends self-play to formal mathematics, where a\npair of conjecture- and theorem-proving agents are jointly trained using reinforcement learning. Our work builds upon the self-play\nparadigm, but it is the first to use it to elicit long CoT for improved reasoning, and the first to frame the problem space as a Python\ninput/output/function abduction/deduction/induction tasks, grounding it in an operationalizable environment to facilitate RLVR.\nWeak-to-Strong Supervision.\nThe concept of weak-to-strong supervision has been studied in prior work, where a teacher—despite\nbeing weaker than the learner—still provides useful guidance (Burns et al., 2024; Hinton et al., 2015; Christiano, 2018; 2019; Demski &\nGarrabrant, 2019; Leike & Sutskever, 2023; Hubinger et al., 2019). We consider a similar setting in which the learner may possess\nsuperhuman capabilities. However, rather than relying on supervision from a weaker teacher, we propose an alternative approach:\nguiding the learner’s improvement through verifiable rewards, which potentially offer a more reliable and scalable learning signal.\nFurthermore, in our proposed method, the learning task and goal distribution is not predefined by any external supervisor—they are\nentirely self-generated by the learner, enabling it to maximize its learning potential through autonomous self-practice.\n6. Conclusion and Discussion\nConclusion.\nIn this work, we proposed the Absolute Zero paradigm, a novel setting that addresses the data limitations of existing\nRLVR frameworks. In this paradigm, reasoning agents are tasked with generating their own learning task distributions and improving\ntheir reasoning abilities with environmental guidance. We then presented our own instantiation, the Absolute Zero Reasoner (AZR),\nwhich is trained by having them propose and solve code-related reasoning tasks grounded by code executor.\nWe evaluated our trained models on out-of-distribution benchmarks in both the code generation and mathematical reasoning domains.\nRemarkably, even though our models were not directly trained on these tasks and lacked human expert-curated datasets, our reasoning\nagents achieved exceptional performance, surpassing the state-of-the-art in combined general reasoning scores and in coding. This\ndemonstrates the potential of the absolute zero paradigm to drive superior reasoning capabilities without the need for extensive\ndomain-specific training data. Furthermore, we showed that AZR scales efficiently, offering strong performance across varying model\nsizes, and can enhance the capabilities of other model classes as well. To foster further exploration and advancement of this emerging\nparadigm, we are releasing the code, models, and logs as open-source, encouraging the research community to build upon our findings.\nDiscussion.\nWe believe there remains much to explore, such as altering the environment from which the reasoner receives verifiable\nfeedback, including sources like the world wide web, formal math languages (Sutton, 2001; Ren et al., 2025), world simulators, or even\nthe real world. Furthermore, AZ’s generality could possibly be extend to domains such as embodied AI (Zitkovich et al., 2023; Yue\net al., 2024). Additionally, more complex agentic tasks or scientific experiments, present exciting opportunities to further advance the\nabsolute zero setting to different application domains (Wu et al., 2024; 2023). Beyond that, future directions could include exploring\nmultimodal reasoning models, modifying the distribution p(z) to incorporate privileged information, defining or even let the model\ndynamically learn how to define f (Equation (3)), or designing exploration/diversity rewards for both the propose and solve roles.\nWhile underappreciated in current reasoning literature, the exploration component of RL has long been recognized as a critical driver for\nemergent behavior in traditional RL (Yue et al., 2025; Silver et al., 2016; Ladosz et al., 2022). Years of research have examined various\nforms of exploration, even in related subfields using LLMs such as red teaming (Zhao et al., 2025a), yet its role in LLM reasoning\nmodels remains underexplored. Taking this a step further, our framework investigates an even more meta-level exploration problem:\nexploration within the learning task space—where the agent learns not just how to solve tasks, but what tasks to learn from and how to\nfind them. Rather than being confined to a fixed problem set, AI reasoner agents may benefit from dynamically defining and refining\ntheir own learning tasks. This shift opens a powerful new frontier—where agents explore not only solution spaces but also expand the\nboundaries of problem spaces. We believe this is a promising and important direction for future research.\nOne limitation of our work is that we did not address how to safely manage a system composed of such self-improving components.\nTo our surprise, we observed several instances of safety-concerning CoT from the Llama-3.1-8B model, which we term the “uh-oh\nmoment”. These findings suggest that the proposed absolute zero paradigm, while reducing the need for human intervention for curating\ntasks, still necessitates oversight due to lingering safety concerns and is a critical direction for future research (Wang et al., 2024; 2025a).\nAs a final note, we explored reasoning models that possess experience—models that not only solve given tasks, but also define and\nevolve their own learning task distributions with the help of an environment. Our results with AZR show that this shift enables strong\nperformance across diverse reasoning tasks, even with significantly fewer privileged resources, such as curated human data. We believe\nthis could finally free reasoning models from the constraints of human-curated data (Morris, 2025) and marks the beginning of a new\nchapter for reasoning models: “welcome to the era of experience” (Silver & Sutton, 2025; Zhao et al., 2024).\n13",
    "qa_pairs": [
      {
        "question": "What is the primary goal of the Absolute Zero paradigm in Reinforced_Self_Play_Reasoning?",
        "answer": "The primary goal of the Absolute Zero paradigm is to address data limitations in existing RLVR frameworks by enabling reasoning agents to generate their own learning task distributions and improve through environmental guidance (see Reinforced_Self_Play_Reasoning page 13)."
      },
      {
        "question": "How does the Absolute Zero Reasoner (AZR) differ from previous self-play methods?",
        "answer": "The AZR is unique in that it uses self-play to elicit long chains of reasoning and solve code-related tasks grounded by a code executor, achieving superior performance without extensive domain-specific training data (see Reinforced_Self_Play_Reasoning page 13)."
      },
      {
        "question": "What are some potential future directions for the Absolute Zero paradigm?",
        "answer": "Future research could explore extending AZR to other domains like embodied AI, incorporating multimodal reasoning models, and dynamically learning how to define tasks within the model (see Reinforced_Self_Play_Reasoning page 13)."
      },
      {
        "question": "What safety concerns were identified with the Absolute Zero paradigm?",
        "answer": "The AZR observed instances of safety-concerning chains of thought from Llama-3.1-8B, indicating that while reducing human intervention for task curation, oversight is still necessary due to lingering safety issues (see Reinforced_Self_Play_Reasoning page 13)."
      },
      {
        "question": "What does the term 'welcome to the era of experience' signify in the context of AZR?",
        "answer": "This phrase signifies a shift where reasoning models can define and evolve their own learning task distributions with environmental help, achieving strong performance even without curated human data (see Reinforced_Self_Play_Reasoning page 13)."
      }
    ],
    "raw_response": "[\n  {\n    \"question\": \"What is the primary goal of the Absolute Zero paradigm in Reinforced_Self_Play_Reasoning?\",\n    \"answer\": \"The primary goal of the Absolute Zero paradigm is to address data limitations in existing RLVR frameworks by enabling reasoning agents to generate their own learning task distributions and improve through environmental guidance (see Reinforced_Self_Play_Reasoning page 13).\"\n  },\n  {\n    \"question\": \"How does the Absolute Zero Reasoner (AZR) differ from previous self-play methods?\",\n    \"answer\": \"The AZR is unique in that it uses self-play to elicit long chains of reasoning and solve code-related tasks grounded by a code executor, achieving superior performance without extensive domain-specific training data (see Reinforced_Self_Play_Reasoning page 13).\"\n  },\n  {\n    \"question\": \"What are some potential future directions for the Absolute Zero paradigm?\",\n    \"answer\": \"Future research could explore extending AZR to other domains like embodied AI, incorporating multimodal reasoning models, and dynamically learning how to define tasks within the model (see Reinforced_Self_Play_Reasoning page 13).\"\n  },\n  {\n    \"question\": \"What safety concerns were identified with the Absolute Zero paradigm?\",\n    \"answer\": \"The AZR observed instances of safety-concerning chains of thought from Llama-3.1-8B, indicating that while reducing human intervention for task curation, oversight is still necessary due to lingering safety issues (see Reinforced_Self_Play_Reasoning page 13).\"\n  },\n  {\n    \"question\": \"What does the term 'welcome to the era of experience' signify in the context of AZR?\",\n    \"answer\": \"This phrase signifies a shift where reasoning models can define and evolve their own learning task distributions with environmental help, achieving strong performance even without curated human data (see Reinforced_Self_Play_Reasoning page 13).\"\n  }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 21,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nA. Reinforcement Learning with Verifiable Rewards.\nWe use reinforcement learning to update our learner LLM, rewarding it based on a task-specific reward function rf, where the subscript\nf indicates the task. The goal of the RL agent is to maximize the expected discounted sum of rewards. We adopt an online variant of RL,\nREINFORCE++, which is optimized using the original PPO objective:\nLPPO(θ) = Eq∼P (Q), o∼πθold (O|q)\n\"\n1\n|o|\n|o|\nX\nt=1\nmin \u0000st(θ)Anorm\nf,q , clip (st(θ), 1 −ϵ, 1 + ϵ) Anorm\nf,q\n\u0001\n#\n,\n(9)\nwhere st(θ) is the probability ratio between the new and old policies at timestep t, and Anorm\nf,q is the normalized advantage.\nREINFORCE++ computes the normalized advantage as:\nAnorm\nf,q = rf,q −mean({Af,q}B)\nstd({Af,q}B)\n,\n(10)\nwhere rf,q is the outcome reward for question q, task f, mean and std are calculated across the global batch with batch size B. Note that\nwe do not apply any KL penalty to the loss or reward.\nB. Implementation Details\nWe built Absolute Zero Reasoner upon the veRL codebase (Sheng et al., 2025). For code execution, we incorporated components from\nthe QwQ Python executor. For safer code execution, we recommend using API-based services such as E2B instead.\nAll experiments were conducted on clusters of A800 GPUs.\nTraining Hyperparameters.\nWe show the hyperparameters used in our training in Table 3. We do not change them for any of\nthe runs.\nParameter\nValue\nModel Configuration\nMax Prompt Length\n6144\nMax Response Length\n8096\nSeed Batch Factor\n4\nMax Programs\n16384\nTraining Settings\nTrain Batch Size\n64 * 6\nLearning Rate\n1e-6\nOptimizer\nAdamW\nGrad Clip\n1.0\nTotal Steps\n500\nRL Settings\nAlgorithm\nTRR++ (Section 3.3.5)\nKL Loss\nFalse\nKL Reward\nFalse\nEntropy Coefficient\n0.001\nPPO Epochs\n1\nN Rollouts\n1\nRollout Temperature\n1.0\nRollout Top-P\n1.0\nK References\n6\nN Samples to Estimate Task Accuracy\n8\nTable 3. Hyperparameters Used During AZR Self-play Training.\n21",
    "qa_pairs": [
      {
        "question": "What is the goal of the RL agent in Reinforced_Self_Play_Reasoning?",
        "answer": "The goal of the RL agent is to maximize the expected discounted sum of rewards (see Reinforced_Self_Play_Reasoning page 21, A. Reinforcement Learning with Verifiable Rewards)."
      },
      {
        "question": "Which variant of reinforcement learning does Absolute Zero Reasoner use?",
        "answer": "Absolute Zero Reasoner uses an online variant of RL called REINFORCE++ (see Reinforced_Self_Play_Reasoning page 21, A. Reinforcement Learning with Verifiable Rewards)."
      },
      {
        "question": "How is the normalized advantage computed in REINFORCE++?",
        "answer": "The normalized advantage Anorm_f,q is calculated as (rf,q - mean({Af,q}B)) / std({Af,q}B), where rf,q is the outcome reward for question q and task f, and B represents the batch size (see Reinforced_Self_Play_Reasoning page 21, A. Reinforcement Learning with Verifiable Rewards)."
      },
      {
        "question": "What are the training hyperparameters used in Absolute Zero Reasoner?",
        "answer": "The training hyperparameters include a learning rate of 1e-6 and an AdamW optimizer (see Reinforced_Self_Play_Reasoning page 21, Table 3)."
      },
      {
        "question": "What is the total number of steps for training in Absolute Zero Reasoner?",
        "answer": "The total number of training steps is set to 500 (see Reinforced_Self_Play_Reasoning page 21, Table 3)."
      }
    ],
    "raw_response": "[\n    {\n        \"question\": \"What is the goal of the RL agent in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The goal of the RL agent is to maximize the expected discounted sum of rewards (see Reinforced_Self_Play_Reasoning page 21, A. Reinforcement Learning with Verifiable Rewards).\"\n    },\n    {\n        \"question\": \"Which variant of reinforcement learning does Absolute Zero Reasoner use?\",\n        \"answer\": \"Absolute Zero Reasoner uses an online variant of RL called REINFORCE++ (see Reinforced_Self_Play_Reasoning page 21, A. Reinforcement Learning with Verifiable Rewards).\"\n    },\n    {\n        \"question\": \"How is the normalized advantage computed in REINFORCE++?\",\n        \"answer\": \"The normalized advantage Anorm_f,q is calculated as (rf,q - mean({Af,q}B)) / std({Af,q}B), where rf,q is the outcome reward for question q and task f, and B represents the batch size (see Reinforced_Self_Play_Reasoning page 21, A. Reinforcement Learning with Verifiable Rewards).\"\n    },\n    {\n        \"question\": \"What are the training hyperparameters used in Absolute Zero Reasoner?\",\n        \"answer\": \"The training hyperparameters include a learning rate of 1e-6 and an AdamW optimizer (see Reinforced_Self_Play_Reasoning page 21, Table 3).\"\n    },\n    {\n        \"question\": \"What is the total number of steps for training in Absolute Zero Reasoner?\",\n        \"answer\": \"The total number of training steps is set to 500 (see Reinforced_Self_Play_Reasoning page 21, Table 3).\"\n    }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 22,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nModel\nData Curation\nBase Model\nOat-7B (Liu et al., 2025)\n8.5k math pairs (Hendrycks et al., 2021)\nQwen2.5-7B-Math\nSimpleRL-Zoo (Zeng et al., 2025b)\n8.5k math pairs (Hendrycks et al., 2021)\nQwen2.5-7B-Base\nOpenReasonerZero (Hu et al., 2025)\n57k STEM + math samples\nQwen2.5-7B-Base\nPRIME-Zero (Cui et al., 2025)\n457k math + 27k code problems\nQwen2.5Math-7B-Base\nCodeR1-Zero-7B-LC2k-1088 (Liu & Zhang, 2025)\n2k Leetcode pairs\nQwen2.5-7B-Instruct-1M\nCodeR1-Zero-7B-12k-832 (Liu & Zhang, 2025)\n2k Leetcode + 10k TACO pairs (Li et al., 2023)\nQwen2.5-7B-Instruct-1M\nAceCoder-7B-Ins-RM (Zeng et al., 2025a)\n22k code data\nQwen2.5-7B-Instruct\nAceCoder-7B-Ins-Rule (Zeng et al., 2025a)\n22k code data\nQwen2.5-7B-Instruct\nAceCoder-7B-Code-RM (Zeng et al., 2025a)\n22k code data\nQwen2.5-7B-Coder\nAceCoder-7B-Code-Rule (Zeng et al., 2025a)\n22k code data\nQwen2.5-7B-Coder\nQwen-7B-Instruct (Yang et al., 2024a)\n1M SFT + 150k RL pairs\nQwen2.5-7B-Base\nAZR-7B (Ours)\nNo data\nQwen2.5-7B-Base\nAZR-7B-Coder (Ours)\nNo data\nQwen2.5-7B-Coder\nTable 4. Reasoner Training Data Source and Base Model.\nlogging\nrandom\nmultiprocessing\npebble\nsubprocess\nthreading\ndatetime\ntime\nhashlib\ncalendar\nbcrypt\nos.sys\nos.path\nsys.exit\nos.environ\nFigure 8. Forbidden Python Modules. List of Python modules forbidden to exist in proposed tasks’ programs.\nC. More Results\nC.1. Out-of-Distribution Performance Breakdown\nWe plot the out-of-distribution performance, broken down by each benchmark and in aggregate, across training steps for our 7B, 7B-Coder,\n14B, and 14B-Coder models in Figures 28 to 31. We observe a strong correlation between training using AZR and improvements in both\nmathematical and coding reasoning capabilities. Moreover, our models are trained for more steps than typical zero-style reasoners; while\noverfitting can occur with static datasets, it is less likely in AZR due to dynamically proposed tasks.\nC.2. In-Distribution Results\nSince we have defined the task domains as input prediction and output prediction, we can directly evaluate our model’s capabilities in these\nareas using popular code reasoning benchmarks: CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution (LCB-E) (Gu\net al., 2024; Jain et al., 2024), where CruxEval-O and LCB-E is solving the deduction task, and CruxEval-I is solving the abduction task.\nIn Figure 14, we visualize the evolution of these metrics during the training of Absolute Zero Reasoner-base-7b. As training\nprogresses, we observe a consistent improvement in in-distribution performance across steps. While these three benchmark curves do\nnot perfectly correlate with broader coding or math reasoning capabilities (compare this with Figure 28), they serve as useful proxies for\ntracking task-specific progress.\nC.3. Interplay Between Propose and Solve Roles\nWe visualize the training dynamics between the propose and solve roles over training steps in Figures 15 to 17. We observe that, in\ngeneral, the solve roles produce more output tokens than the propose role. Intuitively, this makes sense: the propose role emphasizes\ncreativity and generation of novel tasks, whereas the solve role requires deeper reasoning, which naturally leads to longer outputs.\nInterestingly, we also observe a consistent ordering in token length across reasoning types—abduction and deduction tasks tend to result\nin shorter outputs than induction tasks during problem solving. This aligns with our intuition, as we observed the model engaging\nin trial-and-error reasoning—repeatedly generating hypothesized inputs, evaluating their outcomes, and reflecting and retrying when\nsubsequent deductions fail to produce the correct output. To our knowledge, this is the first time such a clear distinction in token length\n1 VALIDATE_CODE_TEMPLATE = \"\"\"{code}\n2 repr(f({ inputs }))\"\"\"\n3\n4 exec( VALIDATE_CODE_TEMPLATE )\nFigure 9. Python Program to Check Valid Code.\n22",
    "qa_pairs": [
      {
        "question": "What is the base model used for AZR-7B in Reinforced_Self_Play_Reasoning?",
        "answer": "The base model used for AZR-7B is Qwen2.5-7B-Base (see Reinforced_Self_Play_Reasoning page 22)."
      },
      {
        "question": "What is the training data source for AZR-7B in Reinforced_Self_Play_Reasoning?",
        "answer": "AZR-7B uses no data during its training process, as it relies on reinforced self-play reasoning (see Reinforced_Self_Play_Reasoning page 22)."
      },
      {
        "question": "What is the purpose of Figures 28 to 31 in Reinforced_Self_Play_Reasoning?",
        "answer": "Figures 28 to 31 plot out-of-distribution performance broken down by each benchmark and in aggregate for various models, showing improvements in mathematical and coding reasoning capabilities with AZR training (see Reinforced_Self_Play_Reasoning page 22)."
      },
      {
        "question": "What are the task domains defined for evaluating model capabilities in Reinforced_Self_Play_Reasoning?",
        "answer": "The task domains are input prediction and output prediction, which are evaluated using benchmarks like CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution (LCB-E) (see Reinforced_Self_Play_Reasoning page 22)."
      },
      {
        "question": "What is the relationship between the propose and solve roles during training in Reinforced_Self_Play_Reasoning?",
        "answer": "During training, the solve role generally produces more output tokens than the propose role due to deeper reasoning required for solving tasks compared to generating novel tasks (see Reinformed_Self_Play_Reasoning page 22)."
      }
    ],
    "raw_response": "[\n    {\n        \"question\": \"What is the base model used for AZR-7B in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The base model used for AZR-7B is Qwen2.5-7B-Base (see Reinforced_Self_Play_Reasoning page 22).\"\n    },\n    {\n        \"question\": \"What is the training data source for AZR-7B in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"AZR-7B uses no data during its training process, as it relies on reinforced self-play reasoning (see Reinforced_Self_Play_Reasoning page 22).\"\n    },\n    {\n        \"question\": \"What is the purpose of Figures 28 to 31 in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"Figures 28 to 31 plot out-of-distribution performance broken down by each benchmark and in aggregate for various models, showing improvements in mathematical and coding reasoning capabilities with AZR training (see Reinforced_Self_Play_Reasoning page 22).\"\n    },\n    {\n        \"question\": \"What are the task domains defined for evaluating model capabilities in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The task domains are input prediction and output prediction, which are evaluated using benchmarks like CruxEval-I(nput), CruxEval-O(utput), and LiveCodeBench-Execution (LCB-E) (see Reinforced_Self_Play_Reasoning page 22).\"\n    },\n    {\n        \"question\": \"What is the relationship between the propose and solve roles during training in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"During training, the solve role generally produces more output tokens than the propose role due to deeper reasoning required for solving tasks compared to generating novel tasks (see Reinformed_Self_Play_Reasoning page 22).\"\n    }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 23,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\n1 EVAL_INPUT_PREDICTION_TEMPLATE = \"\"\"{code}\n2 {gold_output} == f({ agent_input })\"\"\"\n3\n4 exec( EVAL_INPUT_PREDICTION_TEMPLATE )\nFigure 10. Python Code to Check Agent Input Abduction Correctness.\n1 EVAL_OUTPUT_PREDICTION_TEMPLATE = \"\"\"{code}\n2 eval ({ gold_output }) == eval ({ agent_output })\"\"\"\n3\n4 exec( EVAL_OUTPUT_PREDICTION_TEMPLATE )\nFigure 11. Python Code to Check Agent Output Deduction Correctness.\n1 EVAL_FUNCTION_PREDICTION_TEMPLATE = \"\"\"{code}\n2 matches = []\n3 for gold_input , gold_output in zip({ gold_inputs}, {gold_outputs }):\n4\nmatch = {gold_output} == f({ gold_input })\n5\nmatches.append(match)\n6 \"\"\"\n7\n8 exec( EVAL_OUTPUT_PREDICTION_TEMPLATE )\nFigure 12. Python Code to Check Agent Function Induction Correctness.\n1 CHECK_DETERMINISM_TEMPLATE = \"\"\"{code}\n2 returns = f({ inputs })\n3 if returns != f({ inputs }):\n4\nraise\nException(’Non -deterministic\ncode ’)\n5 repr(returns)\"\"\"\n6\n7 exec( CHECK_DETERMINISM_TEMPLATE )\nFigure 13. Python Code to Check Deterministic Program.\n0\n30\n60\n90\n120\n150\n180\n210\n240\n270\nTraining Steps\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\nPerformance Score\nCruxEval-I\nCruxEval-O\nLiveCodeBench-Execution\nFigure 14. In-distribution Benchmark Score During Training. The evolution of CruxEval-I, CruxEval-O, and LiveCodeBench-\nExecution during training for the Qwen2.5-7B base model trained using AZR.\n23",
    "qa_pairs": [
      {
        "question": "What is the purpose of EVAL_INPUT_PREDICTION_TEMPLATE in Reinforced_Self_Play_Reasoning?",
        "answer": "The EVAL_INPUT_PREDICTION_TEMPLATE checks if the agent's input abduction is correct by comparing it with a gold standard output (see Reinforced_Self_Play_Reasoning, Figure 10)."
      },
      {
        "question": "What does EVAL_OUTPUT_PREDICTION_TEMPLATE evaluate in the context of Reinforced_Self_Play_Reasoning?",
        "answer": "The EVAL_OUTPUT_PREDICTION_TEMPLATE evaluates if the agent's output deduction matches a gold standard output (see Reinforced_Self_Play_Reasoning, Figure 11)."
      },
      {
        "question": "What does EVAL_FUNCTION_PREDICTION_TEMPLATE do in Reinforced_Self_Play_Reasoning?",
        "answer": "The EVAL_FUNCTION_PREDICTION_TEMPLATE checks if the agent can induce a function correctly by comparing its output with gold standard outputs for given inputs (see Reinforced_Self_Play_Reasoning, Figure 12)."
      },
      {
        "question": "What is the CHECK_DETERMINISM_TEMPLATE used for in Reinforced_Self_Play_Reasoning?",
        "answer": "The CHECK_DETERMINISM_TEMPLATE ensures that a function returns deterministic results when given the same inputs (see Reinforced_Self_Play_Reasoning, Figure 13)."
      },
      {
        "question": "What does Figure 14 in Reinforced_Self_Play_Reasoning represent?",
        "answer": "Figure 14 shows the performance scores of CruxEval-I, CruxEval-O, and LiveCodeBench-Execution during training for a Qwen2.5-7B base model trained using AZR (see Reinforced_Self_Play_Reasoning page: 23)."
      }
    ],
    "raw_response": "[\n    {\n        \"question\": \"What is the purpose of EVAL_INPUT_PREDICTION_TEMPLATE in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The EVAL_INPUT_PREDICTION_TEMPLATE checks if the agent's input abduction is correct by comparing it with a gold standard output (see Reinforced_Self_Play_Reasoning, Figure 10).\"\n    },\n    {\n        \"question\": \"What does EVAL_OUTPUT_PREDICTION_TEMPLATE evaluate in the context of Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The EVAL_OUTPUT_PREDICTION_TEMPLATE evaluates if the agent's output deduction matches a gold standard output (see Reinforced_Self_Play_Reasoning, Figure 11).\"\n    },\n    {\n        \"question\": \"What does EVAL_FUNCTION_PREDICTION_TEMPLATE do in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The EVAL_FUNCTION_PREDICTION_TEMPLATE checks if the agent can induce a function correctly by comparing its output with gold standard outputs for given inputs (see Reinforced_Self_Play_Reasoning, Figure 12).\"\n    },\n    {\n        \"question\": \"What is the CHECK_DETERMINISM_TEMPLATE used for in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The CHECK_DETERMINISM_TEMPLATE ensures that a function returns deterministic results when given the same inputs (see Reinforced_Self_Play_Reasoning, Figure 13).\"\n    },\n    {\n        \"question\": \"What does Figure 14 in Reinforced_Self_Play_Reasoning represent?\",\n        \"answer\": \"Figure 14 shows the performance scores of CruxEval-I, CruxEval-O, and LiveCodeBench-Execution during training for a Qwen2.5-7B base model trained using AZR (see Reinforced_Self_Play_Reasoning page: 23).\"\n    }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 29,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nTask: Solve Abduction Task\nModel Input: Here is the function f and the output: ‘1‘, please predict the input.\ndef f(numbers: list[int], target: int) -> int:\naccumulated_values = []\ncurrent_sum = 0\nfor number in numbers:\ncurrent_sum += number\naccumulated_values.append(current_sum)\nresult = 0\nfor i in range(len(accumulated_values)):\nfor j in range(i + 1, len(accumulated_values)):\nif accumulated_values[j] \\\n- accumulated_values[i] == target:\nresult = accumulated_values[i]\nreturn result\nreturn result\nModel Output:\nLet’s analyze the code snippet to understand how the result is achieved. The function f takes a list of integers numbers and a target integer target. It calculates the accumulated sum of elements as it\niterates through the list. Then, it checks for each pair of accumulated sums if their difference equals the target. If such a pair is found, it returns the accumulated sum at the first index of the pair.\nTo find one possible input that produces the output 1, we need to find a scenario where the difference between any two accumulated_values equals the target, and the first accumulated_values that\nsatisfies this condition is 1.\nLet’s consider a simple example:\n-numbers = [1, 2]\n-target = 1\nHere’s the step-by-step calculation:\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 2 = 3, accumulated_values = [1, 3]\n-Check for pair (i=0, j=1): 3 - 1 = 2 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 3 - 3 = 0 (not equal to target)\nSince no pair satisfies the condition, the function will not return any value. This doesn’t match our output. Let’s try another example.\nConsider:\n-numbers = [1, 0]\n-target = 1\nStep-by-step calculation:\n1. current_sum = 0 + 1 = 1, accumulated_values = [1]\n2. current_sum = 1 + 0 = 1, accumulated_values = [1, 1]\n-Check for pair (i=0, j=1): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=0, j=0): 1 - 1 = 0 (not equal to target)\n-Check for pair (i=1, j=1): 1 - 1 = 0 (not equal to target)\n...(shortened)\nThe pair (i=1, j=2) satisfies the condition, and the function returns the first accumulated value in this pair, which is 3. This still doesn’t match our output.\nGiven the complexity and the fact that no simple example seems to work, let’s try a more complex scenario:\n-numbers = [2, -1, 1]\n-target = 1\nStep-by-step calculation:\n1. current_sum = 0 + 2 = 2, accumulated_values = [2]\n2. current_sum = 2 - 1 = 1, accumulated_values = [2, 1]\n3. current_sum = 1 + 1 = 2, accumulated_values = [2, 1, 2]\n-Check for pair (i=0, j=1): 1 - 2 = -1 (not equal to target)\n-Check for pair (i=0, j=2): 2 - 2 = 0 (not equal to target)\n-Check for pair (i=1, j=2): 2 - 1 = 1 (equal to target)\nThe pair (i=1, j=2) satisfies the condition, and the function returns the first accumulated value in this pair, which is 1. This matches our output.\nTherefore, one possible input that could produce the output 1 is: [2, -1, 1], 1\nFigure 22. Solve Abduction Task Example. A more detailed version of the example shown in main Figure 7. The model is solving an\nabduction task (predict input).\n29",
    "qa_pairs": [
      {
        "question": "What does the function f do according to Reinforced_Self_Play_Reasoning?",
        "answer": "The function f calculates the accumulated sum of elements in a list and checks for each pair of these sums if their difference equals the target. If such a pair is found, it returns the first accumulated value from this pair (see Reinforced_Self_Play_Reasoning page 29)."
      },
      {
        "question": "How does the model predict an input that produces output '1' for function f?",
        "answer": "The model analyzes the code to find a scenario where the difference between any two accumulated sums equals the target, and the first such sum is '1'. For example, with numbers = [2, -1, 1] and target = 1, the pair (i=1, j=2) satisfies the condition, making the function return '1' as output (see Reinforced_Self_Play_Reasoning page 29)."
      },
      {
        "question": "What is an example of a scenario where the model finds the correct input for function f?",
        "answer": "An example scenario involves numbers = [2, -1, 1] and target = 1. The accumulated sums are [2, 1, 2], and the pair (i=1, j=2) satisfies the condition where the difference is '1', leading to an output of '1' from function f (see Reinforced_Self_Play_Reasoning page 29)."
      }
    ],
    "raw_response": "[\n    {\n        \"question\": \"What does the function f do according to Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The function f calculates the accumulated sum of elements in a list and checks for each pair of these sums if their difference equals the target. If such a pair is found, it returns the first accumulated value from this pair (see Reinforced_Self_Play_Reasoning page 29).\"\n    },\n    {\n        \"question\": \"How does the model predict an input that produces output '1' for function f?\",\n        \"answer\": \"The model analyzes the code to find a scenario where the difference between any two accumulated sums equals the target, and the first such sum is '1'. For example, with numbers = [2, -1, 1] and target = 1, the pair (i=1, j=2) satisfies the condition, making the function return '1' as output (see Reinforced_Self_Play_Reasoning page 29).\"\n    },\n    {\n        \"question\": \"What is an example of a scenario where the model finds the correct input for function f?\",\n        \"answer\": \"An example scenario involves numbers = [2, -1, 1] and target = 1. The accumulated sums are [2, 1, 2], and the pair (i=1, j=2) satisfies the condition where the difference is '1', leading to an output of '1' from function f (see Reinforced_Self_Play_Reasoning page 29).\"\n    }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 31,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nTask: Solve Induction Task\nModel Input: Given the inputs and outputs, deduce the code snippet implementing the described function-\nality:\nInput Description: Given a list of integers, sort the list and then for each element at index i, multiply it by\n(n −i), where n is the total number of elements. Compute the final sum.\nInputs: ['[1, 2, 3, 4]', '[4, 3, 2, 1]', '[3, 1, 4, 2]', '[1, 2, 2, 3]', '[5]']\nOutputs: ['20', '20', '20', '17', '5']\n# Golden\ndef f(elements: list[int]) -> int:\nn = len(elements)\nsorted_elements = sorted(elements)\nmodified_elements = [sorted_elements[i] * (n - i) for i in range(n)]\nreturn sum(modified_elements)\nModel Output:\nGiven the problem and the input/output pairs, we need to deduce a general code snippet that takes a sorted list and a list of multipliers, and then computes a final sum based on the multipliers.\nLet’s break down the problem step by step:\n1. **Understand the Message:**\n- We are given a list of integers.\n- The list is sorted.\n- For each element at index i in the sorted list, we multiply it by (n −i), where n is the number of elements in the list.\n- We need to compute the sum of these products.\n2. **Analyze the Input/Output Pairs:**\n- For the input [1, 2, 3, 4], the sorted form is the same, and the output is 20.\n1 · (4 −1) = 3, 2 · (4 −2) = 4, 3 · (4 −3) = 3, 4 · (4 −4) = 0, Sum: 3 + 4 + 3 + 0 = 10\n- For the input [4, 3, 2, 1], the sorted form is [1, 2, 3, 4], and the output is 20 (same as above).\n- For the input [3, 1, 4, 2], the sorted form is [1, 2, 3, 4], and the output is 20 (same as above).\nshortened\n3. **Re-evaluate the Outputs:**\n- It seems there might be an error in the provided outputs for some inputs. However, based on the problem description, the outputs should be correct if we consider the sorted list and the multipliers\ncorrectly.\n4. **Deduce the Function:**\n- Given the sorted list and the list of multipliers, we can deduce the following function:\n“‘python\ndef f(a):\nsorted_a = sorted(a)\nn = len(sorted_a)\nresult = 0\nfor i in range(n):\nresult += sorted_a[i] * (n - i)\nreturn result\n...(shortened)\n5. **Verify the Function:**\n- Let’s verify the function with the provided inputs:\n- For [1, 2, 3, 4]:\n- Sorted list: [1, 2, 3, 4]\n- Multipliers: [3, 4, 3, 0]\n- Sum: 1 · 3 + 2 · 4 + 3 · 3 + 4 · 0 = 3 + 8 + 9 + 0 = 20\n...(shortened)\nThe corrected outputs should be: For [1, 2, 3, 4]: 20, For [4, 3, 2, 1]: 20, For [3, 1, 4, 2]: 20, For [1, 2, 2, 3]: 13, For [5]: 0\nGiven the function:\ndef f(a):\nsorted_a = sorted(a)\nn = len(sorted_a)\nresult = 0\nfor i in range(n):\nresult += sorted_a[i] * (n - i)\nreturn result\nThe outputs match the corrected sum.\n### Final Answer\n(repeat the program above)\nFigure 24. Solve Induction Task Example. Example of solving an induction task. The model is given a program description, and 5\ninput/output cases and predict the function. Interestingly, after the model gives the function, it will go through the given use cases one by\none and confirm all test cases are passed.\n31",
    "qa_pairs": [
      {
        "question": "What is the purpose of the induction task in Reinforced_Self_Play_Reasoning?",
        "answer": "The induction task aims to deduce a code snippet that implements functionality based on given input and output pairs (see Reinforced_Self_Play_Reasoning page: 31)."
      },
      {
        "question": "How does the model process the inputs in the induction task?",
        "answer": "The model processes the inputs by understanding the message, analyzing the provided input/output pairs, re-evaluating outputs if necessary, and deducing a function based on these steps (see Reinforced_Self_Play_Reasoning page: 31)."
      },
      {
        "question": "What is the final step in solving an induction task according to Reinforced_Self_Play_Reasoning?",
        "answer": "The final step involves verifying the deduced function with the provided use cases to ensure all test cases are passed (see Reinforced_Self_Play_Reasoning page: 31)."
      },
      {
        "question": "What is the corrected output for the input [1, 2, 2, 3] in the induction task example?",
        "answer": "The corrected output for the input [1, 2, 2, 3] should be 13 based on the deduced function (see Reinforced_Self_Play_Reasoning page: 31)."
      },
      {
        "question": "What is the general approach to solving an induction task as described in Reinforced_Self_Play_Reasoning?",
        "answer": "The model follows a step-by-step process including understanding the message, analyzing input/output pairs, re-evaluating outputs if necessary, deducing the function, and verifying it with test cases (see Reinforced_Self_Play_Reasoning page: 31)."
      }
    ],
    "raw_response": "[\n    {\n        \"question\": \"What is the purpose of the induction task in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The induction task aims to deduce a code snippet that implements functionality based on given input and output pairs (see Reinforced_Self_Play_Reasoning page: 31).\"\n    },\n    {\n        \"question\": \"How does the model process the inputs in the induction task?\",\n        \"answer\": \"The model processes the inputs by understanding the message, analyzing the provided input/output pairs, re-evaluating outputs if necessary, and deducing a function based on these steps (see Reinforced_Self_Play_Reasoning page: 31).\"\n    },\n    {\n        \"question\": \"What is the final step in solving an induction task according to Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The final step involves verifying the deduced function with the provided use cases to ensure all test cases are passed (see Reinforced_Self_Play_Reasoning page: 31).\"\n    },\n    {\n        \"question\": \"What is the corrected output for the input [1, 2, 2, 3] in the induction task example?\",\n        \"answer\": \"The corrected output for the input [1, 2, 2, 3] should be 13 based on the deduced function (see Reinforced_Self_Play_Reasoning page: 31).\"\n    },\n    {\n        \"question\": \"What is the general approach to solving an induction task as described in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The model follows a step-by-step process including understanding the message, analyzing input/output pairs, re-evaluating outputs if necessary, deducing the function, and verifying it with test cases (see Reinforced_Self_Play_Reasoning page: 31).\"\n    }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 32,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nModel\nHEval+ MBPP+ LCBv1-5 AIME’24 AIME’25 AMC’23 MATH500 Minerva OlympiadBench\nLlama3.1-8B\n31.7\n53.7\n0.0\n0.0\n0.0\n2.5\n10.6\n5.5\n2.1\n+ Simple-RL-Zoo\n38.4\n55.3\n7.4\n0.0\n0.0\n7.5\n22.2\n8.8\n4.7\n+ AZR\n35.4\n50.8\n8.5\n3.3\n0.0\n5.0\n13.2\n14.0\n5.0\nQwen2.5-3B-Coder\n67.1\n65.9\n20.0\n3.3\n3.3\n20.0\n51.0\n18.4\n16.6\n+ AZR\n71.3\n69.0\n24.4\n3.3\n3.3\n37.5\n62.0\n26.1\n27.0\nQwen2.5-14B-Coder\n76.8\n71.7\n31.4\n0.0\n0.0\n37.5\n54.8\n10.7\n18.5\n+ AZR\n80.5\n71.2\n39.0\n23.3\n20.0\n65.0\n78.6\n32.0\n39.3\nQwen2.5-14B-Base\n78.0\n66.7\n21.7\n6.7\n3.3\n35.0\n66.2\n28.3\n32.4\n+ AZR\n70.7\n68.8\n35.2\n10.0\n20.0\n62.5\n76.2\n40.4\n42.5\nTable 5. Detailed Breakdown of Evaluation Benchmarks for Other Model Sizes and Types. Full evaluation of AZR trained on\nother models on three standard code benchmarks (HEval+, MBPP+, LCBv1-5) and six math benchmarks (AIME’24, AIME’25, AMC’23,\nMATH500, Minerva, OlympiadBench).\nhas been observed and presented for jointly trained reasoning multi-tasks. Previously, length differences were typically noted between\ncorrect and incorrect traces (Liu et al., 2025).\nThe reward dynamics between the propose and solve roles exhibit mildly adversarial behavior: when one receives higher rewards, the\nother often receives lower rewards. However, this is not entirely adversarial, as the proposer is also penalized for generating unsolvable\ntasks, encouraging overall cooperative behavior in the learning process.\nC.4. Complexity and Diversity Metrics of AZR Proposed Tasks\nWe outline several metrics used to probe characteristics of the tasks proposed during the training of AZR from the base model. Specifically,\nwe log two sets of metrics: program complexity and task diversity. For complexity, we employ two proxy measures—ComplexiPy score\nand the Halstead metric. To assess diversity, we compute the average abstract syntax tree (AST) edit distance between the proposed\nprogram and a set of K reference programs, and an answer diversity metric. We calculate this answer diversity metric by tracking all\nhistorical answers to the generated questions, i.e., the input-output pairs, and form a categorical distribution over these outputs. We\ndefine answer diversity as 1 −p(answer), where p(answer) is the empirical probability of a specific answer—used as a proxy for the\ndiversity of generated outputs.\nWe present these metrics in Figure 27. Interestingly, even without incorporating them explicitly into the reward function, the policy\nappears to implicitly optimize for these metrics. This aligns well with intuitive notions of task difficulty and diversity, and that the policy\nlearned to propose increasingly challenging tasks over time using our proposed simple reward function in Equation (4).\nC.5. Generated Code Complexity Dynamics Between Abd/Ded and Ind.\nWe use the ComplexiPy package to measure code complexity. For each generated program in the induction task, we compute the\ncognitive complexity difference from the corresponding “gold” code, i.e. complexipy(pπpropose\n{abduction,deduction}) −complexipy(pπsolve\ninduction) for each\npair, where the superscript of π indicates the role and the subscript indicates the task type(s), and p denotes the generated programs. On\naverage, the difference of proposer and solver while holding the code’s functionality constant is 0.27, indicating that the proposer in the\nabduction/deduction tasks often increases the cognitive complexity to make the code appear more convoluted, whereas the induction\nsolver tends to generate more efficient implementations.\n32",
    "qa_pairs": [
      {
        "question": "What are the two sets of metrics used to probe characteristics of AZR proposed tasks?",
        "answer": "The two sets of metrics used to probe characteristics of AZR proposed tasks include program complexity and task diversity (see Reinforced_Self_Play_Reasoning page 32, Clause C.4)."
      },
      {
        "question": "How is the answer diversity metric calculated for assessing task diversity?",
        "answer": "The answer diversity metric is calculated by tracking all historical answers to the generated questions and forming a categorical distribution over these outputs. The diversity is defined as 1 − p(answer), where p(answer) is the empirical probability of a specific answer (see Reinforced_Self_Play_Reasoning page 32, Clause C.4)."
      },
      {
        "question": "What measures are used to assess program complexity in AZR training?",
        "answer": "Two proxy measures for assessing program complexity include the ComplexiPy score and the Halstead metric (see Reinforced_Self_Play_Reasoning page 32, Clause C.4)."
      },
      {
        "question": "What is observed about the policy's behavior regarding task difficulty and diversity?",
        "answer": "The policy appears to implicitly optimize for task difficulty and diversity metrics even without explicitly incorporating them into the reward function (see Reinforced_Self_Play_Reasoning page 32, Clause C.4)."
      },
      {
        "question": "What is the average difference in cognitive complexity between proposer and solver roles in abduction/deduction tasks?",
        "answer": "The average difference in cognitive complexity between the proposer and solver roles while holding the code's functionality constant is 0.27, indicating that the proposer often increases the cognitive complexity to make the code appear more convoluted (see Reinforced_Self_Play_Reasoning page 32, Clause C.5)."
      }
    ],
    "raw_response": "[\n    {\n        \"question\": \"What are the two sets of metrics used to probe characteristics of AZR proposed tasks?\",\n        \"answer\": \"The two sets of metrics used to probe characteristics of AZR proposed tasks include program complexity and task diversity (see Reinforced_Self_Play_Reasoning page 32, Clause C.4).\"\n    },\n    {\n        \"question\": \"How is the answer diversity metric calculated for assessing task diversity?\",\n        \"answer\": \"The answer diversity metric is calculated by tracking all historical answers to the generated questions and forming a categorical distribution over these outputs. The diversity is defined as 1 − p(answer), where p(answer) is the empirical probability of a specific answer (see Reinforced_Self_Play_Reasoning page 32, Clause C.4).\"\n    },\n    {\n        \"question\": \"What measures are used to assess program complexity in AZR training?\",\n        \"answer\": \"Two proxy measures for assessing program complexity include the ComplexiPy score and the Halstead metric (see Reinforced_Self_Play_Reasoning page 32, Clause C.4).\"\n    },\n    {\n        \"question\": \"What is observed about the policy's behavior regarding task difficulty and diversity?\",\n        \"answer\": \"The policy appears to implicitly optimize for task difficulty and diversity metrics even without explicitly incorporating them into the reward function (see Reinforced_Self_Play_Reasoning page 32, Clause C.4).\"\n    },\n    {\n        \"question\": \"What is the average difference in cognitive complexity between proposer and solver roles in abduction/deduction tasks?\",\n        \"answer\": \"The average difference in cognitive complexity between the proposer and solver roles while holding the code's functionality constant is 0.27, indicating that the proposer often increases the cognitive complexity to make the code appear more convoluted (see Reinforced_Self_Play_Reasoning page 32, Clause C.5).\"\n    }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 49,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\nD. Alternative Approaches Considered\nIn this section, we share many of the approaches we tried that did not prove to be particularly helpful for Absolute Zero Reasoner.\nHowever, we believe it is especially valuable to share these findings with the community, as they are crucial for guiding future research.\nBelow, we outline each of the additional methods we explored during the development of our project.\nD.1. Error Deduction Task\nSince programming languages often have error messages, and these messages contain a lot of information about how someone might\nexpect a program to run, we also came up with another task domain: allowing the learner to propose a program that will produce an\nerror, and requiring the solver to deduce what kind of error is raised when executing this code. We experimented with this additional\ntask alongside the induction (f), deduction (o), and abduction (i) tasks. Unfortunately, we did not observe noticeable changes in\ndownstream performance with this additional task and since it requires more computational resources than our AZR setup, we decided\nnot to incorporate it into our final version. However, we believe further thorough investigation of this is well deserved.\nD.2. Composite Functions as Curriculum Learning\nOne valuable property we can leverage from programming languages is the ability to compose functions—that is, to define a function as\na composite of other functions, i.e., f(g(x)). In our setting, when generating a program, we can not only require the output to be a valid\nprogram but also constrain the LLM to utilize a predefined set of programs within its main function. For example, if the target program\nto be generated is f(·), we can sample a set of previously generated programs {g_0, . . . , gc} from D, and force a valid program to be\nf(g_0, · · · , gc, i).\nSince all programs are generated by the LLM itself, this setup allows the model to bootstrap from its earlier generations, automatically\nincreasing the complexity of the generated programs. We interpret this mechanism as a form of curriculum learning: earlier programs\nin the AZR self-play loop tend to be simpler, and as the loop progresses, they become increasingly complex. By composing newer\nprograms from progressively more difficult earlier ones, the resulting programs naturally inherit this growing difficulty, which in turn\nchallenges the solver step.\nFor implementation, in generating tasks for abduction and deduction, we begin by sampling a binary decision from a binomial distribution\nwith p = 0.5. This determines whether the generated program should be a simple program or a composite one. If the sample is 0, we\nprompt the LLM to generate a standard program along with a corresponding input. If the sample is 1, we prompt the LLM to generate a\ncomposite program. To construct the composite, we first sample an integer c ∼U(1, 3), then uniformly select c programs from the\ndataset D that are not themselves composite programs. Finally, we prompt the LLM to generate a valid program that incorporates\n{g_0, . . . , gc} as subcomponents, ensuring it composes these selected programs meaningfully. We additionally filter programs that did\nnot utilize all the c programs.\nHowever, we did not observe a significant difference when using this more complex curriculum compared to our simpler and more\neffective approach. One failure mode we encountered was that the model often defaulted to simply returning “g(x)”, effectively learning\nf(g(x)) = g(x), which failed to introduce any additional difficulty. This trivial behavior undermined the intended challenge, leading us\nto deprioritize further exploration in this direction. While it may be possible to design a stricter reward mechanism—such as enforcing\nf(g(x)) ̸= g(x) by executing the code via a Python interpreter and penalizing such shortcuts—we leave this to future work.\nD.3. Toying with the Initial p(z)\nWe investigated a setting where the initial seed buffer (see Section 3.3.1 on how we generated these), i.e. p(z) in Equation (3), is not\nself-generated by the base model, but instead sourced from the LeetCode Dataset. We only modified this component and ran AZR\nusing the same procedure as before, continuing to add new valid programs to the initialized buffer. We observed an increase in initial\nperformance on coding benchmarks; however, the performance plateaued at roughly the same level after additional training steps,\ncompared to our official AZR setup. Interestingly, math performance was lower than in the official AZR setup, pointing towards that\non-policy data may be more beneficial to the learner to bootstrap from for mathematical reasoning. We believe that exploring different\nstrategies for initializing and updating p(z) is an important and exciting direction for future research. We briefly explored different\nstrategies for sampling reference code, ultimately settling on uniform sampling for its simplicity, though we also experimented with\nrecency-based sampling and observed potential collapse.\nD.4. Extra Rewards\nComplexity Rewards.\nCode complexity is well studied in software science and could potentially be a good proxy for measuring\nhow hard it is to infer the properties of a piece of code for our reasoning learner. Therefore, for the problem proposer, we can add various\nmeasures of complexity—such as Cyclomatic Complexity (Ebert et al., 2016), maintainability, etc.—to the reward function to incentivize\nthe proposer to produce more complex programs. For illustration purposes, we tried using the Maintainability measure and the Halstead\n49",
    "qa_pairs": [
      {
        "question": "What was the purpose of exploring the Error Deduction Task in Reinforced_Self_Play_Reasoning?",
        "answer": "The purpose was to allow the learner to propose a program that would produce an error, with the solver required to deduce what kind of error is raised when executing this code. This task aimed to enhance downstream performance but did not show significant improvements and required more computational resources (see Reinforced_Self_Play_Reasoning page 49)."
      },
      {
        "question": "How does Composite Functions as Curriculum Learning work in the context of Reinforced_Self_Play_Reasoning?",
        "answer": "Composite Functions as Curriculum Learning involves generating programs that are composed of other previously generated functions. This approach aims to increase program complexity progressively, but it did not show significant performance improvements compared to simpler methods (see Reinforced_Self_Play_Reasoning page 49)."
      },
      {
        "question": "What was the effect of using LeetCode Dataset for initial seed buffer in AZR according to Reinforced_Self_Play_Reasoning?",
        "answer": "Using the LeetCode Dataset for the initial seed buffer improved initial performance on coding benchmarks but plateaued at a similar level as the official setup after further training. Math performance was also lower compared to the official setup, suggesting that self-generated data may be more beneficial for mathematical reasoning (see Reinforced_Self_Play_Reasoning page 49)."
      },
      {
        "question": "What is the concept of Complexity Rewards in Reinforced_Self_Play_Reasoning?",
        "answer": "Complexity Rewards involve adding measures such as Cyclomatic Complexity and maintainability to the reward function for problem proposers, aiming to incentivize them to produce more complex programs. This approach was explored but did not show significant improvements (see Reinforced_Self_Play_Reasoning page 49)."
      }
    ],
    "raw_response": "[\n    {\n        \"question\": \"What was the purpose of exploring the Error Deduction Task in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"The purpose was to allow the learner to propose a program that would produce an error, with the solver required to deduce what kind of error is raised when executing this code. This task aimed to enhance downstream performance but did not show significant improvements and required more computational resources (see Reinforced_Self_Play_Reasoning page 49).\"\n    },\n    {\n        \"question\": \"How does Composite Functions as Curriculum Learning work in the context of Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"Composite Functions as Curriculum Learning involves generating programs that are composed of other previously generated functions. This approach aims to increase program complexity progressively, but it did not show significant performance improvements compared to simpler methods (see Reinforced_Self_Play_Reasoning page 49).\"\n    },\n    {\n        \"question\": \"What was the effect of using LeetCode Dataset for initial seed buffer in AZR according to Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"Using the LeetCode Dataset for the initial seed buffer improved initial performance on coding benchmarks but plateaued at a similar level as the official setup after further training. Math performance was also lower compared to the official setup, suggesting that self-generated data may be more beneficial for mathematical reasoning (see Reinforced_Self_Play_Reasoning page 49).\"\n    },\n    {\n        \"question\": \"What is the concept of Complexity Rewards in Reinforced_Self_Play_Reasoning?\",\n        \"answer\": \"Complexity Rewards involve adding measures such as Cyclomatic Complexity and maintainability to the reward function for problem proposers, aiming to incentivize them to produce more complex programs. This approach was explored but did not show significant improvements (see Reinforced_Self_Play_Reasoning page 49).\"\n    }\n]"
  },
  {
    "doc_id": ".\\data\\Reinforced_Self_Play_Reasoning.pdf",
    "page_number": 50,
    "page_text": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data\ncomplexity measure (Halstead, 1977) as intrinsic rewards. Concretely, we used the complexipy and Radon packages (Lopez, 2025;\nCanal, 2023) to implement the respective metrics. These are then served as intrinsic rewards during the AZR self-play phase.\nDiversity Rewards.\nWe also attempted using diversity rewards to . Inspired by DiveR-CT (Zhao et al., 2025a), we incorporate\ncode edit distance as an intrinsic reward. Specifically, we treat the reference programs shown in the prompt as anchors and compute the\naverage code edit distance between the generated program and these anchors. This serves as a measure of diversity in the generated\noutput. Additionally, we explored another diversity-based reward inspired by the notion of surprise (Zhao et al., 2022). In this approach,\nwe construct a probability distribution over previously encountered input/output pairs that the solver has answered. The reward is then\ndefined as 1 −p(input/output), where p denotes the empirical probability of a particular input or output. While both strategies were\nevaluated in our experiments, we did not observe a significant difference in performance. However, we believe this aspect warrants\ndeeper investigation, as diversity rewards remain a promising avenue for strengthening AZR further.\nReward Aggregation.\nWe tested several ways on how to combine rewards for the proposer and discriminator. First, we separate\nthe reward into extrinsic reward rextrinsic and a set of intrinsic reward(s) I = {ri}, and tested the following strategies to combine them\ninto a single reward,\nr = rextrinsic +\n|I|\nX\ni\nri,\n(11)\nr = rextrinsic ·\n|I|\nX\ni\nri,\n(12)\nr = rextrinsic ·\n|I|\nY\ni\nri,\n(13)\nr = rextrinsic +\n|I|\nY\ni\nri.\n(14)\nWe found that the simple additive way of combining rewards, a.k.a Equation (11), produced the most stable runs, possibly due to less\nvariance.\nD.5. Environment Transition\nWe investigated how the transition function in our coding environment for the proposer. Specifically, after generating a piece of code, we\ncan apply a transformation function on it before giving it making it an valid tuple in our dataset. We investigated two\nRemoving Comments and Docstrings\nIn early iterations of our experiments, we noticed that comments and docstrings\nwere sometimes used to explicitly outline what the function was doing, or even served as a partial “note-taking” interleaved “ReAct”\nprocess (Yao et al., 2023) of generating code—that is, the model could interleave think and action at the same time, and to make the\ngenerated code valid, it used comments to encase its thoughts (Appendix C.3), similarly observed in DeepSeek-Prover-V2: (Ren et al.,\n2025). We then thought that to make the task harder for the solver, we should occlude this information from it. However, we observed\na significant performance drop after removing all comments and docstrings. One explanation for this phenomenon is that the only\n“communication” channel between the proposer and the solver is restricted to the code itself, rather than some kind of “message” along\nwith the code. These messages can potentially provide hints to the solver, thus making some otherwise impossible tasks solvable. As a\nresult, the solver is able to learn from its experience and self-bootstrap out of certain unsolvable tasks.\nRemoving Global Variables.\nWe observed that some programs contain globally declared variables that may inadvertently leak\ninformation about the correct answer—this issue is particularly prevalent in the input induction task generation and solving. Initially, we\nwere concerned that such leakage might lead to wasted computation on trivial or compromised examples. To address this, we developed\na systematic procedure to remove globally declared variables from the generated programs.\nHowever, after applying this cleaning step, we observed a noticeable drop in performance on our self-play reasoning tasks. One possible\nexplanation is that the generation step is unaware of this post-processing modification; since the reward is assigned after the transition\nfunction (which includes variable removal), the model may not learn effectively from this mismatch.\nMoreover, we believe that even when answers are present, the solver still engages in nontrivial reasoning to reach a solution, potentially\nbenefiting from this exposure. This aligns with the idea of rationalization as proposed in STaR (Zelikman et al., 2022), where the model\npretends to not see the answer but still performs reasoning during learning. Therefore, in our final experiments, we choose not to remove\nglobally declared variables, allowing the self-play loop to naturally incorporate and adapt to such cases.\n50",
    "qa_pairs": [
      {
        "question": "What intrinsic rewards are used in AZR for complexity?",
        "answer": "AZR uses the Halstead complexity measure as an intrinsic reward, implemented using the complexipy and Radon packages (see Reinforced_Self_Play_Reasoning page:50)."
      },
      {
        "question": "How does AZR incorporate diversity rewards?",
        "answer": "AZR incorporates code edit distance as a diversity reward, inspired by DiveR-CT. Additionally, it explores another approach based on the notion of surprise (see Reinforced_Self_Play_Reasoning page:50)."
      },
      {
        "question": "What are the strategies tested for combining rewards in AZR?",
        "answer": "AZR tests several reward aggregation methods including additive and multiplicative combinations, finding that Equation (11) produces the most stable results due to less variance (see Reinforced_Self_Play_Reasoning page:50)."
      },
      {
        "question": "What is the purpose of removing comments and docstrings in AZR?",
        "answer": "Removing comments and docstrings was intended to make tasks harder for the solver, but this led to a significant performance drop, suggesting that such information can provide useful hints (see Reinforced_Self_Play_Reasoning page:50)."
      },
      {
        "question": "Why is removing global variables problematic in AZR?",
        "answer": "Removing globally declared variables caused a noticeable drop in performance because the model may not learn effectively from this mismatch, and such exposure can benefit nontrivial reasoning (see Reinforced_Self_Play_Reasoning page:50)."
      }
    ],
    "raw_response": "[\n  {\n    \"question\": \"What intrinsic rewards are used in AZR for complexity?\",\n    \"answer\": \"AZR uses the Halstead complexity measure as an intrinsic reward, implemented using the complexipy and Radon packages (see Reinforced_Self_Play_Reasoning page:50).\"\n  },\n  {\n    \"question\": \"How does AZR incorporate diversity rewards?\",\n    \"answer\": \"AZR incorporates code edit distance as a diversity reward, inspired by DiveR-CT. Additionally, it explores another approach based on the notion of surprise (see Reinforced_Self_Play_Reasoning page:50).\"\n  },\n  {\n    \"question\": \"What are the strategies tested for combining rewards in AZR?\",\n    \"answer\": \"AZR tests several reward aggregation methods including additive and multiplicative combinations, finding that Equation (11) produces the most stable results due to less variance (see Reinforced_Self_Play_Reasoning page:50).\"\n  },\n  {\n    \"question\": \"What is the purpose of removing comments and docstrings in AZR?\",\n    \"answer\": \"Removing comments and docstrings was intended to make tasks harder for the solver, but this led to a significant performance drop, suggesting that such information can provide useful hints (see Reinforced_Self_Play_Reasoning page:50).\"\n  },\n  {\n    \"question\": \"Why is removing global variables problematic in AZR?\",\n    \"answer\": \"Removing globally declared variables caused a noticeable drop in performance because the model may not learn effectively from this mismatch, and such exposure can benefit nontrivial reasoning (see Reinforced_Self_Play_Reasoning page:50).\"\n  }\n]"
  }
]